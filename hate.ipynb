{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "mount_file_id": "1mdyaiJgIINv0TY4bTWVh0-KdFXCNLmZ2",
      "authorship_tag": "ABX9TyMxCmUXQfzBWsAwlJkOP5yD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Stevebankz/Hate_Speech_Detection/blob/main/hate.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Cell 1: Install Dependencies ---\n",
        "\n",
        "print(\"Installing required libraries...\")\n",
        "!pip install transformers datasets scikit-learn -q\n",
        "\n",
        "print(\"--- Cell 1 Complete ---\")\n",
        "\n",
        "\n",
        "# --- Cell 2: Import Libraries & Mount Google Drive ---\n",
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "from datasets import load_dataset, DatasetDict\n",
        "\n",
        "# Import individual metric functions\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.utils import resample\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    EarlyStoppingCallback\n",
        ")\n",
        "import gc # Garbage collector\n",
        "\n",
        "# Mount your Google Drive\n",
        "# This will prompt you for authorization.\n",
        "#print(\"Mounting Google Drive...\")\n",
        "#drive.mount('/content/drive')\n",
        "\n",
        "print(\"--- Cell 2 Complete ---\")\n",
        "\n",
        "\n",
        "# --- Cell 3: Define Configuration and Paths ---\n",
        "# We'll set all our paths and hyperparameters here.\n",
        "# This makes it super easy to change things later.\n",
        "\n",
        "class Config:\n",
        "    # --- Paths ---\n",
        "    # This is the base path in your Google Drive\n",
        "    DRIVE_PATH = '/content/drive/MyDrive/hate'\n",
        "\n",
        "    # --- IMPORTANT ---\n",
        "    # Change 'csv' to 'json' or 'parquet' if your files are not CSVs\n",
        "    DATA_FILE_TYPE = 'csv'\n",
        "\n",
        "    # Paths to your data files\n",
        "    TRAIN_FILE = os.path.join(DRIVE_PATH, 'train.csv')\n",
        "    VAL_FILE = os.path.join(DRIVE_PATH, 'val.csv')\n",
        "    TEST_FILE = os.path.join(DRIVE_PATH, 'test.csv')\n",
        "\n",
        "    # Where we will save the trained model\n",
        "    MODEL_SAVE_PATH = os.path.join(DRIVE_PATH, 'models/step1_bert_baseline')\n",
        "\n",
        "    # --- Model Configuration ---\n",
        "    # Since your data is multilingual, we CANNOT use 'bert-base-uncased'.\n",
        "    # We MUST use a multilingual model. 'bert-base-multilingual-cased' (mBERT)\n",
        "    # is the standard and perfect for this baseline.\n",
        "    MODEL_NAME = 'bert-base-multilingual-cased'\n",
        "\n",
        "    # --- Training Hyperparameters ---\n",
        "    MAX_LENGTH = 128  # Max token length for sentences\n",
        "    BATCH_SIZE = 16   # Batch size for training and eval\n",
        "    EPOCHS = 5        # Number of training epochs (5 is a good start)\n",
        "    LEARNING_RATE = 2e-5 # Standard learning rate for fine-tuning BERT\n",
        "\n",
        "    # --- Labels ---\n",
        "    NUM_LABELS = 2 # 0 (Non-hate) and 1 (Hate)\n",
        "\n",
        "    # --- Evaluation ---\n",
        "    N_BOOTSTRAPS = 1000 # Number of bootstrap samples for CIs\n",
        "\n",
        "print(\"Configuration defined.\")\n",
        "print(f\"Model to be trained: {Config.MODEL_NAME}\")\n",
        "print(f\"Model will be saved to: {Config.MODEL_SAVE_PATH}\")\n",
        "print(f\"Will run {Config.N_BOOTSTRAPS} bootstrap iterations for CI.\")\n",
        "print(\"--- Cell 3 Complete ---\")\n",
        "\n",
        "\n",
        "# --- Cell 4: Check for GPU ---\n",
        "# Let's make sure we're using a GPU. Colab notebooks should have one.\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f\"Awesome! We are using the GPU: {torch.cuda.get_device_name(0)}\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"No GPU found. We are using the CPU (this will be SLOW).\")\n",
        "\n",
        "print(\"--- Cell 4 Complete ---\")\n",
        "\n",
        "\n",
        "# --- Cell 5: Load Dataset ---\n",
        "# We use the 'datasets' library to load our split files directly\n",
        "# into a DatasetDict object.\n",
        "\n",
        "print(f\"Loading data from {Config.DRIVE_PATH}...\")\n",
        "try:\n",
        "    data_files = {\n",
        "        'train': Config.TRAIN_FILE,\n",
        "        'validation': Config.VAL_FILE,\n",
        "        'test': Config.TEST_FILE\n",
        "    }\n",
        "\n",
        "\n",
        "    raw_datasets = load_dataset(Config.DATA_FILE_TYPE, data_files=data_files)\n",
        "\n",
        "    print(\"Data loaded successfully!\")\n",
        "    print(raw_datasets)\n",
        "\n",
        "    # Let's see an example\n",
        "    print(\"\\nExample from training set:\")\n",
        "    print(raw_datasets['train'][0])\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"--- ERROR LOADING DATA ---\")\n",
        "    print(f\"Could not load data. Check your paths and file type ('{Config.DATA_FILE_TYPE}').\")\n",
        "    print(f\"Error: {e}\")\n",
        "\n",
        "    raise\n",
        "\n",
        "print(\"--- Cell 5 Complete ---\")\n",
        "\n",
        "\n",
        "# --- Cell 6: Preprocessing (Tokenization) ---\n",
        "# convert our 'text' into numbers (tokens) .\n",
        "\n",
        "print(f\"Loading tokenizer for {Config.MODEL_NAME}...\")\n",
        "# We use AutoTokenizer to automatically load the correct one for mBERT\n",
        "tokenizer = AutoTokenizer.from_pretrained(Config.MODEL_NAME)\n",
        "\n",
        "# This function will be applied to our entire dataset\n",
        "def tokenize_function(batch):\n",
        "    # 'text' is your text column.\n",
        "    # 'padding=\"max_length\"' pads all sentences to 128 tokens.\n",
        "    # 'truncation=True' cuts off sentences longer than 128 tokens.\n",
        "    return tokenizer(\n",
        "        batch['text'],\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        max_length=Config.MAX_LENGTH\n",
        "    )\n",
        "\n",
        "print(\"Tokenizing datasets... (this may take a minute)\")\n",
        "\n",
        "# use .map() to apply the tokenization function to all splits\n",
        "# batched=True makes it much faster.\n",
        "# remove the original columns we don't need for training.\n",
        "tokenized_datasets = raw_datasets.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    remove_columns=['post_id', 'text', 'label_name', 'label_3class', 'targets'] # Remove all non-essential columns\n",
        ")\n",
        "\n",
        "# The Trainer expects the label column to be named 'labels'\n",
        "# rename our 'label' column\n",
        "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
        "\n",
        "# Set the format to 'torch' so it returns PyTorch tensors\n",
        "tokenized_datasets.set_format('torch')\n",
        "\n",
        "print(\"Tokenization complete.\")\n",
        "print(tokenized_datasets)\n",
        "print(\"\\nExample of processed data:\")\n",
        "print(tokenized_datasets['train'][0])\n",
        "\n",
        "print(\"--- Cell 6 Complete ---\")\n",
        "\n",
        "\n",
        "# --- Cell 7: Load Baseline Model ---\n",
        "# load the mBERT model, configured for sequence classification.\n",
        "\n",
        "print(f\"Loading pre-trained model: {Config.MODEL_NAME}...\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    Config.MODEL_NAME,\n",
        "    num_labels=Config.NUM_LABELS # 2 labels: 0 (Non-hate) and 1 (Hate)\n",
        ")\n",
        "\n",
        "# Move the model to the GPU\n",
        "model.to(device)\n",
        "print(\"Model loaded and moved to GPU.\")\n",
        "print(\"--- Cell 7 Complete ---\")\n",
        "\n",
        "\n",
        "# --- Cell 8: Define Evaluation Metrics ---\n",
        "# This function is passed to the Trainer.\n",
        "# It calculates the F1, Precision, and Recall .\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    \"\"\"\n",
        "    Called by the Trainer at evaluation time.\n",
        "    \"\"\"\n",
        "    # eval_pred is a tuple of (logits, labels)\n",
        "    logits, labels = eval_pred\n",
        "\n",
        "    # Get the most likely prediction (index with the highest logit)\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "\n",
        "    # Calculate metrics using individual functions\n",
        "    # We use 'macro' averaging as it's good for potentially imbalanced datasets\n",
        "    # and standard for classification tasks.\n",
        "    precision = precision_score(labels, predictions, average='macro')\n",
        "    recall = recall_score(labels, predictions, average='macro')\n",
        "    f1 = f1_score(labels, predictions, average='macro')\n",
        "\n",
        "\n",
        "    acc = accuracy_score(labels, predictions)\n",
        "\n",
        "    # Return as a dictionary\n",
        "    return {\n",
        "        'accuracy': acc,\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall\n",
        "    }\n",
        "\n",
        "print(\"Metrics function 'compute_metrics' defined.\")\n",
        "print(\"--- Cell 8 Complete ---\")\n",
        "\n",
        "\n",
        "# --- Cell 9: Configure Training Arguments ---\n",
        "# This object holds all the training settings.\n",
        "\n",
        "print(\"Configuring training arguments...\")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=Config.MODEL_SAVE_PATH,\n",
        "\n",
        "    # --- Training Hyperparameters ---\n",
        "    num_train_epochs=Config.EPOCHS,\n",
        "    learning_rate=Config.LEARNING_RATE,\n",
        "    per_device_train_batch_size=Config.BATCH_SIZE,\n",
        "    per_device_eval_batch_size=Config.BATCH_SIZE * 2,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "\n",
        "    # --- Evaluation and Saving ---\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1\",\n",
        "    greater_is_better=True,\n",
        "\n",
        "    # --- Logging (silence step logs, keep only tqdm bars) ---\n",
        "    report_to=\"none\",\n",
        "    logging_strategy=\"no\",     # <- no step/epoch log lines\n",
        "    disable_tqdm=False         # <- keep progress bars\n",
        ")\n",
        "\n",
        "\n",
        "print(\"--- Cell 9 Complete ---\")\n",
        "\n",
        "\n",
        "# --- Cell 10: Initialize Trainer ---\n",
        "# The Trainer class handles all the complexity of training and evaluation.\n",
        "\n",
        "print(\"Initializing Trainer...\")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,                         # The model we just loaded\n",
        "    args=training_args,                  # The training arguments we just defined\n",
        "    train_dataset=tokenized_datasets[\"train\"], # Our tokenized training data\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],  # Our tokenized validation data\n",
        "    tokenizer=tokenizer,                 # The tokenizer (so it can be saved with the model)\n",
        "    compute_metrics=compute_metrics      # The function to calculate our metrics\n",
        ")\n",
        "\n",
        "print(\"Trainer initialized.\")\n",
        "print(\"--- Cell 10 Complete ---\")\n",
        "\n",
        "\n",
        "# --- Cell 11: Train the Model ---\n",
        "#  We call .train() to start fine-tuning.\n",
        "\n",
        "print(\"--- STARTING BASELINE MODEL TRAINING ---\")\n",
        "print(f\"Training for {Config.EPOCHS} epochs...\")\n",
        "\n",
        "training_results = trainer.train()\n",
        "\n",
        "print(\"--- TRAINING COMPLETE ---\")\n",
        "print(\"--- Cell 11 Complete ---\")\n",
        "\n",
        "\n",
        "# --- Cell 12: Save the Best Model and Results ---\n",
        "\n",
        "\n",
        "print(f\"Saving the best model to {Config.MODEL_SAVE_PATH}...\")\n",
        "\n",
        "# This saves the model, tokenizer, and config files\n",
        "trainer.save_model(Config.MODEL_SAVE_PATH)\n",
        "\n",
        "# We'll also save the training results\n",
        "trainer.save_state()\n",
        "\n",
        "print(f\"Model successfully saved to {Config.MODEL_SAVE_PATH}\")\n",
        "print(\"--- Cell 12 Complete ---\")\n",
        "\n",
        "\n",
        "# --- Cell 13: Evaluate on the TEST Set (with Bootstrap CIs) ---\n",
        "\n",
        "\n",
        "print(\"--- EVALUATING ON THE TEST SET (SINGLE PASS) ---\")\n",
        "\n",
        "# We run one clean pass to get the point-estimate\n",
        "clean_test_results = trainer.evaluate(tokenized_datasets[\"test\"])\n",
        "\n",
        "print(\"\\n\\n--- FINAL BASELINE MODEL TEST RESULTS (CLEAN) ---\")\n",
        "print(f\"Model: {Config.MODEL_NAME}\")\n",
        "print(f\"Test F1-Score:   {clean_test_results['eval_f1']:.4f}\")\n",
        "print(f\"Test Accuracy:   {clean_test_results['eval_accuracy']:.4f}\")\n",
        "print(f\"Test Precision:  {clean_test_results['eval_precision']:.4f}\")\n",
        "print(f\"Test Recall:     {clean_test_results['eval_recall']:.4f}\")\n",
        "print(\"---------------------------------------------------\\n\")\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "print(f\"--- STARTING BOOTSTRAP EVALUATION ({Config.N_BOOTSTRAPS} iterations) ---\")\n",
        "\n",
        "test_dataset = tokenized_datasets[\"test\"]\n",
        "n_samples = len(test_dataset)\n",
        "boot_f1_scores = []\n",
        "boot_accuracy_scores = []\n",
        "boot_precision_scores = []\n",
        "boot_recall_scores = []\n",
        "\n",
        "for _ in tqdm(range(Config.N_BOOTSTRAPS), desc=\"Bootstrapping\", leave=False):\n",
        "    boot_indices = resample(range(n_samples), replace=True, n_samples=n_samples)\n",
        "    boot_sample = test_dataset.select(boot_indices)\n",
        "    boot_results = trainer.evaluate(boot_sample, metric_key_prefix=\"boot\")\n",
        "    boot_f1_scores.append(boot_results['boot_f1'])\n",
        "    boot_accuracy_scores.append(boot_results['boot_accuracy'])\n",
        "    boot_precision_scores.append(boot_results['boot_precision'])\n",
        "    boot_recall_scores.append(boot_results['boot_recall'])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"--- BOOTSTRAP EVALUATION COMPLETE ---\")\n",
        "\n",
        "# Convert lists to numpy arrays for percentile calculation\n",
        "boot_f1_scores = np.array(boot_f1_scores)\n",
        "boot_accuracy_scores = np.array(boot_accuracy_scores)\n",
        "boot_precision_scores = np.array(boot_precision_scores)\n",
        "boot_recall_scores = np.array(boot_recall_scores)\n",
        "\n",
        "# Calculate 95% confidence intervals (from 2.5th to 97.5th percentile)\n",
        "f1_ci = np.percentile(boot_f1_scores, [2.5, 97.5])\n",
        "acc_ci = np.percentile(boot_accuracy_scores, [2.5, 97.5])\n",
        "prec_ci = np.percentile(boot_precision_scores, [2.5, 97.5])\n",
        "rec_ci = np.percentile(boot_recall_scores, [2.5, 97.5])\n",
        "\n",
        "# Calculate means\n",
        "f1_mean = np.mean(boot_f1_scores)\n",
        "acc_mean = np.mean(boot_accuracy_scores)\n",
        "prec_mean = np.mean(boot_precision_scores)\n",
        "rec_mean = np.mean(boot_recall_scores)\n",
        "\n",
        "print(\"\\n\\n--- FINAL BASELINE MODEL TEST RESULTS (BOOTSTRAPPED) ---\")\n",
        "print(f\"Metrics based on {Config.N_BOOTSTRAPS} bootstrap samples.\")\n",
        "print(f\"Format: Mean (95% CI)\")\n",
        "print(\"----------------------------------------------------------\")\n",
        "print(f\"Test F1-Score:   {f1_mean:.4f} (95% CI: [{f1_ci[0]:.4f}, {f1_ci[1]:.4f}])\")\n",
        "print(f\"Test Accuracy:   {acc_mean:.4f} (95% CI: [{acc_ci[0]:.4f}, {acc_ci[1]:.4f}])\")\n",
        "print(f\"Test Precision:  {prec_mean:.4f} (95% CI: [{prec_ci[0]:.4f}, {prec_ci[1]:.4f}])\")\n",
        "print(f\"Test Recall:     {rec_mean:.4f} (95% CI: [{rec_ci[0]:.4f}, {rec_ci[1]:.4f}])\")\n",
        "print(\"----------------------------------------------------------\\n\")\n",
        "\n",
        "\n",
        "# save these results to a file for our records\n",
        "results_file = os.path.join(Config.DRIVE_PATH, 'models', 'step1_baseline_results.txt')\n",
        "with open(results_file, 'w') as f:\n",
        "    f.write(\"--- FINAL BASELINE MODEL TEST RESULTS ---\\n\\n\")\n",
        "    f.write(f\"Model: {Config.MODEL_NAME}\\n\\n\")\n",
        "\n",
        "    f.write(\"--- SINGLE PASS (CLEAN) RESULTS ---\\n\")\n",
        "    f.write(f\"Test F1-Score:   {clean_test_results['eval_f1']:.4f}\\n\")\n",
        "    f.write(f\"Test Accuracy:   {clean_test_results['eval_accuracy']:.4f}\\n\")\n",
        "    f.write(f\"Test Precision:  {clean_test_results['eval_precision']:.4f}\\n\")\n",
        "    f.write(f\"Test Recall:     {clean_test_results['eval_recall']:.4f}\\n\\n\")\n",
        "\n",
        "    f.write(f\"--- BOOTSTRAPPED RESULTS ({Config.N_BOOTSTRAPS} samples) ---\\n\")\n",
        "    f.write(f\"Format: Mean (95% CI)\\n\")\n",
        "    f.write(f\"Test F1-Score:   {f1_mean:.4f} (95% CI: [{f1_ci[0]:.4f}, {f1_ci[1]:.4f}])\\n\")\n",
        "    f.write(f\"Test Accuracy:   {acc_mean:.4f} (95% CI: [{acc_ci[0]:.4f}, {acc_ci[1]:.4f}])\\n\")\n",
        "    f.write(f\"Test Precision:  {prec_mean:.4f} (95% CI: [{prec_ci[0]:.4f}, {prec_ci[1]:.4f}])\\n\")\n",
        "    f.write(f\"Test Recall:     {rec_mean:.4f} (95% CI: [{rec_ci[0]:.4f}, {rec_ci[1]:.4f}])\\n\")\n",
        "\n",
        "\n",
        "print(f\"Test results saved to {results_file}\")\n",
        "print(\"--- Cell 13 Complete ---\")\n",
        "\n",
        "\n",
        "# --- Cell 14: Clean Up Memory ---\n",
        "\n",
        "\n",
        "print(\"Cleaning up memory...\")\n",
        "del model\n",
        "del trainer\n",
        "del tokenized_datasets\n",
        "del raw_datasets\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "print(\"--- STEP 1 COMPLETE ---\")\n",
        "print(\"You now have a trained, saved, and evaluated baseline model.\")"
      ],
      "metadata": {
        "id": "umJXMbHxXTMr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Cell 1: Install Dependencies ---\n",
        "print(\"Installing required libraries...\")\n",
        "!pip install transformers datasets scikit-learn -q\n",
        "\n",
        "print(\"--- Cell 1 Complete ---\")\n",
        "\n",
        "\n",
        "# --- Cell 2: Import Libraries & Mount Google Drive ---\n",
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "from datasets import load_dataset, DatasetDict\n",
        "# Import individual metric functions\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.utils import resample\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    AutoModel,\n",
        "    AutoConfig,\n",
        "    PreTrainedModel,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    EarlyStoppingCallback\n",
        ")\n",
        "import torch.nn as nn\n",
        "import gc  # Garbage collector\n",
        "\n",
        "# (Leave Drive mount commented if you don't need it right now)\n",
        "# print(\"Mounting Google Drive...\")\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "print(\"--- Cell 2 Complete ---\")\n",
        "\n",
        "\n",
        "# --- Cell 3: Define Configuration and Paths ---\n",
        "class Config:\n",
        "    # --- Paths ---\n",
        "    DRIVE_PATH = '/content/drive/MyDrive/hate'\n",
        "\n",
        "    # --- File type ---\n",
        "    DATA_FILE_TYPE = 'csv'\n",
        "\n",
        "    # Paths to your data files\n",
        "    TRAIN_FILE = os.path.join(DRIVE_PATH, 'train.csv')\n",
        "    VAL_FILE = os.path.join(DRIVE_PATH, 'val.csv')\n",
        "    TEST_FILE = os.path.join(DRIVE_PATH, 'test.csv')\n",
        "\n",
        "    # Save path: use a distinct folder for the gated model\n",
        "    MODEL_SAVE_PATH = os.path.join(DRIVE_PATH, 'models/step1_gated_fusion')\n",
        "\n",
        "    # --- Model Configuration ---\n",
        "    # BASE model for tokenizer & encoder\n",
        "    BASE_MODEL_NAME = 'bert-base-multilingual-cased'\n",
        "    # Public-facing name for this experiment\n",
        "    MODEL_NAME = 'gated'\n",
        "\n",
        "    # --- Training Hyperparameters ---\n",
        "    MAX_LENGTH = 128\n",
        "    BATCH_SIZE = 16\n",
        "    EPOCHS = 5\n",
        "    LEARNING_RATE = 2e-5\n",
        "\n",
        "    # --- Labels ---\n",
        "    NUM_LABELS = 2  # 0 (Non-hate), 1 (Hate)\n",
        "\n",
        "    # --- Evaluation ---\n",
        "    N_BOOTSTRAPS = 1000\n",
        "\n",
        "print(\"Configuration defined.\")\n",
        "print(f\"Model to be trained: {Config.MODEL_NAME}\")\n",
        "print(f\"Base encoder: {Config.BASE_MODEL_NAME}\")\n",
        "print(f\"Model will be saved to: {Config.MODEL_SAVE_PATH}\")\n",
        "print(f\"Will run {Config.N_BOOTSTRAPS} bootstrap iterations for CI.\")\n",
        "print(\"--- Cell 3 Complete ---\")\n",
        "\n",
        "\n",
        "# --- Cell 4: Check for GPU ---\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f\"Awesome! We are using the GPU: {torch.cuda.get_device_name(0)}\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"No GPU found. We are using the CPU (this will be SLOW).\")\n",
        "\n",
        "print(\"--- Cell 4 Complete ---\")\n",
        "\n",
        "\n",
        "# --- Cell 5: Load Dataset ---\n",
        "print(f\"Loading data from {Config.DRIVE_PATH}...\")\n",
        "try:\n",
        "    data_files = {\n",
        "        'train': Config.TRAIN_FILE,\n",
        "        'validation': Config.VAL_FILE,\n",
        "        'test': Config.TEST_FILE\n",
        "    }\n",
        "    raw_datasets = load_dataset(Config.DATA_FILE_TYPE, data_files=data_files)\n",
        "\n",
        "    print(\"Data loaded successfully!\")\n",
        "    print(raw_datasets)\n",
        "\n",
        "    print(\"\\nExample from training set:\")\n",
        "    print(raw_datasets['train'][0])\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"--- ERROR LOADING DATA ---\")\n",
        "    print(f\"Could not load data. Check your paths and file type ('{Config.DATA_FILE_TYPE}').\")\n",
        "    print(f\"Error: {e}\")\n",
        "    raise\n",
        "\n",
        "print(\"--- Cell 5 Complete ---\")\n",
        "\n",
        "\n",
        "# --- Cell 6: Preprocessing (Tokenization) ---\n",
        "print(f\"Loading tokenizer for {Config.BASE_MODEL_NAME}...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(Config.BASE_MODEL_NAME)\n",
        "\n",
        "def tokenize_function(batch):\n",
        "    return tokenizer(\n",
        "        batch['text'],\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        max_length=Config.MAX_LENGTH\n",
        "    )\n",
        "\n",
        "print(\"Tokenizing datasets... (this may take a minute)\")\n",
        "tokenized_datasets = raw_datasets.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    remove_columns=['post_id', 'text', 'label_name', 'label_3class', 'targets']\n",
        ")\n",
        "\n",
        "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
        "tokenized_datasets.set_format('torch')\n",
        "\n",
        "print(\"Tokenization complete.\")\n",
        "print(tokenized_datasets)\n",
        "print(\"\\nExample of processed data:\")\n",
        "print(tokenized_datasets['train'][0])\n",
        "\n",
        "print(\"--- Cell 6 Complete ---\")\n",
        "\n",
        "\n",
        "# --- Cell 7: Gated-Fusion Model (fixed & drop-in) ---\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoModel, AutoConfig, PreTrainedModel, PretrainedConfig\n",
        "\n",
        "class GFConfig(PretrainedConfig):\n",
        "    \"\"\"Config that can be safely constructed with no args by HF internals.\"\"\"\n",
        "    model_type = \"gated_fusion_wrapper\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        base_model_name: str = \"bert-base-multilingual-cased\",\n",
        "        num_labels: int = 2,\n",
        "        gate_hidden: int = 256,\n",
        "        **kwargs\n",
        "    ):\n",
        "        super().__init__(**kwargs)\n",
        "        self.base_model_name = base_model_name\n",
        "        self.num_labels = num_labels\n",
        "        self.gate_hidden = gate_hidden\n",
        "\n",
        "class GatedFusionForSequenceClassification(PreTrainedModel):\n",
        "    config_class = GFConfig\n",
        "\n",
        "    def __init__(self, config: GFConfig):\n",
        "        super().__init__(config)\n",
        "        self.base_cfg = AutoConfig.from_pretrained(config.base_model_name)\n",
        "        self.encoder = AutoModel.from_pretrained(config.base_model_name, config=self.base_cfg)\n",
        "        hidden = self.base_cfg.hidden_size\n",
        "\n",
        "        # Gate over hidden dims using [CLS] and masked-mean pooled token reps\n",
        "        self.gate_mlp = nn.Sequential(\n",
        "            nn.Linear(2 * hidden, config.gate_hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(config.gate_hidden, hidden),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        self.dropout = nn.Dropout(getattr(self.base_cfg, \"hidden_dropout_prob\", 0.1))\n",
        "        self.classifier = nn.Linear(hidden, config.num_labels)\n",
        "\n",
        "        self.post_init()\n",
        "\n",
        "    @staticmethod\n",
        "    def masked_mean(last_hidden_state, attention_mask):\n",
        "        # attention_mask: [B, L], last_hidden_state: [B, L, H]\n",
        "        mask = attention_mask.unsqueeze(-1).type_as(last_hidden_state)  # [B, L, 1]\n",
        "        summed = (last_hidden_state * mask).sum(dim=1)                  # [B, H]\n",
        "        denom = mask.sum(dim=1).clamp(min=1e-6)                         # [B, 1]\n",
        "        return summed / denom\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        labels=None,\n",
        "        **kwargs\n",
        "    ):\n",
        "        # Drop Trainer-injected / unknown kwargs that the base model won't accept\n",
        "        allowed = {\n",
        "            \"position_ids\", \"head_mask\", \"inputs_embeds\",\n",
        "            \"output_attentions\", \"output_hidden_states\", \"return_dict\",\n",
        "            \"past_key_values\", \"encoder_hidden_states\", \"encoder_attention_mask\"\n",
        "        }\n",
        "        safe_kwargs = {k: v for k, v in kwargs.items() if k in allowed}\n",
        "        safe_kwargs.pop(\"num_items_in_batch\", None)\n",
        "\n",
        "        enc = self.encoder(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            **safe_kwargs\n",
        "        )\n",
        "\n",
        "        # CLS and masked mean pooling\n",
        "        h_cls = enc.last_hidden_state[:, 0, :]                           # [B, H]\n",
        "        h_mean = self.masked_mean(enc.last_hidden_state, attention_mask) # [B, H]\n",
        "\n",
        "        # Gated fusion\n",
        "        gate_inp = torch.cat([h_cls, h_mean], dim=-1)                    # [B, 2H]\n",
        "        g = self.gate_mlp(gate_inp)                                      # [B, H] in (0,1)\n",
        "        fused = g * h_cls + (1.0 - g) * h_mean\n",
        "        fused = self.dropout(fused)\n",
        "        logits = self.classifier(fused)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(logits.view(-1, self.config.num_labels), labels.view(-1))\n",
        "\n",
        "        return {\"loss\": loss, \"logits\": logits}\n",
        "\n",
        "print(f\"Loading gated-fusion head on base encoder: {Config.BASE_MODEL_NAME}\")\n",
        "gf_config = GFConfig(\n",
        "    base_model_name=Config.BASE_MODEL_NAME,\n",
        "    num_labels=Config.NUM_LABELS,\n",
        "    gate_hidden=256\n",
        ")\n",
        "model = GatedFusionForSequenceClassification(gf_config).to(device)\n",
        "print(\"Gated-fusion model loaded and moved to device.\")\n",
        "print(\"--- Cell 7 Complete ---\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# --- Cell 8: Define Evaluation Metrics ---\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    precision = precision_score(labels, predictions, average='macro')\n",
        "    recall = recall_score(labels, predictions, average='macro')\n",
        "    f1 = f1_score(labels, predictions, average='macro')\n",
        "    acc = accuracy_score(labels, predictions)\n",
        "    return {'accuracy': acc, 'f1': f1, 'precision': precision, 'recall': recall}\n",
        "\n",
        "print(\"Metrics function 'compute_metrics' defined.\")\n",
        "print(\"--- Cell 8 Complete ---\")\n",
        "\n",
        "\n",
        "# --- Cell 9: Configure Training Arguments ---\n",
        "print(\"Configuring training arguments...\")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=Config.MODEL_SAVE_PATH,\n",
        "\n",
        "    # --- Training Hyperparameters ---\n",
        "    num_train_epochs=Config.EPOCHS,\n",
        "    learning_rate=Config.LEARNING_RATE,\n",
        "    per_device_train_batch_size=Config.BATCH_SIZE,\n",
        "    per_device_eval_batch_size=Config.BATCH_SIZE * 2,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "\n",
        "    # --- Evaluation and Saving ---\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1\",\n",
        "    greater_is_better=True,\n",
        "\n",
        "    # --- Logging (progress bars only) ---\n",
        "    report_to=\"none\",\n",
        "    logging_strategy=\"no\",\n",
        "    disable_tqdm=False\n",
        ")\n",
        "\n",
        "print(\"--- Cell 9 Complete ---\")\n",
        "\n",
        "\n",
        "# --- Cell 10: Initialize Trainer ---\n",
        "print(\"Initializing Trainer...\")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "print(\"Trainer initialized.\")\n",
        "print(\"--- Cell 10 Complete ---\")\n",
        "\n",
        "\n",
        "# --- Cell 11: Train the Model ---\n",
        "print(\"--- STARTING GATED MODEL TRAINING ---\")\n",
        "print(f\"Training for {Config.EPOCHS} epochs...\")\n",
        "\n",
        "training_results = trainer.train()\n",
        "\n",
        "print(\"--- TRAINING COMPLETE ---\")\n",
        "print(\"--- Cell 11 Complete ---\")\n",
        "\n",
        "\n",
        "# --- Cell 12: Save the Best Model and Results ---\n",
        "print(f\"Saving the best model to {Config.MODEL_SAVE_PATH}...\")\n",
        "trainer.save_model(Config.MODEL_SAVE_PATH)\n",
        "trainer.save_state()\n",
        "print(f\"Model successfully saved to {Config.MODEL_SAVE_PATH}\")\n",
        "print(\"--- Cell 12 Complete ---\")\n",
        "\n",
        "\n",
        "# --- Cell 13: Evaluate on the TEST Set (with Bootstrap CIs) ---\n",
        "print(\"--- EVALUATING ON THE TEST SET (SINGLE PASS) ---\")\n",
        "\n",
        "clean_test_results = trainer.evaluate(tokenized_datasets[\"test\"])\n",
        "\n",
        "print(\"\\n\\n--- FINAL GATED MODEL TEST RESULTS (CLEAN) ---\")\n",
        "print(f\"Model: {Config.MODEL_NAME}\")\n",
        "print(f\"Test F1-Score:   {clean_test_results['eval_f1']:.4f}\")\n",
        "print(f\"Test Accuracy:   {clean_test_results['eval_accuracy']:.4f}\")\n",
        "print(f\"Test Precision:  {clean_test_results['eval_precision']:.4f}\")\n",
        "print(f\"Test Recall:     {clean_test_results['eval_recall']:.4f}\")\n",
        "print(\"---------------------------------------------------\\n\")\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "print(f\"--- STARTING BOOTSTRAP EVALUATION ({Config.N_BOOTSTRAPS} iterations) ---\")\n",
        "\n",
        "test_dataset = tokenized_datasets[\"test\"]\n",
        "n_samples = len(test_dataset)\n",
        "boot_f1_scores = []\n",
        "boot_accuracy_scores = []\n",
        "boot_precision_scores = []\n",
        "boot_recall_scores = []\n",
        "\n",
        "for _ in tqdm(range(Config.N_BOOTSTRAPS), desc=\"Bootstrapping\", leave=False):\n",
        "    boot_indices = resample(range(n_samples), replace=True, n_samples=n_samples)\n",
        "    boot_sample = test_dataset.select(boot_indices)\n",
        "    boot_results = trainer.evaluate(boot_sample, metric_key_prefix=\"boot\")\n",
        "    boot_f1_scores.append(boot_results['boot_f1'])\n",
        "    boot_accuracy_scores.append(boot_results['boot_accuracy'])\n",
        "    boot_precision_scores.append(boot_results['boot_precision'])\n",
        "    boot_recall_scores.append(boot_results['boot_recall'])\n",
        "\n",
        "print(\"--- BOOTSTRAP EVALUATION COMPLETE ---\")\n",
        "\n",
        "boot_f1_scores = np.array(boot_f1_scores)\n",
        "boot_accuracy_scores = np.array(boot_accuracy_scores)\n",
        "boot_precision_scores = np.array(boot_precision_scores)\n",
        "boot_recall_scores = np.array(boot_recall_scores)\n",
        "\n",
        "f1_ci = np.percentile(boot_f1_scores, [2.5, 97.5])\n",
        "acc_ci = np.percentile(boot_accuracy_scores, [2.5, 97.5])\n",
        "prec_ci = np.percentile(boot_precision_scores, [2.5, 97.5])\n",
        "rec_ci = np.percentile(boot_recall_scores, [2.5, 97.5])\n",
        "\n",
        "f1_mean = np.mean(boot_f1_scores)\n",
        "acc_mean = np.mean(boot_accuracy_scores)\n",
        "prec_mean = np.mean(boot_precision_scores)\n",
        "rec_mean = np.mean(boot_recall_scores)\n",
        "\n",
        "print(\"\\n\\n--- FINAL GATED MODEL TEST RESULTS (BOOTSTRAPPED) ---\")\n",
        "print(f\"Metrics based on {Config.N_BOOTSTRAPS} bootstrap samples.\")\n",
        "print(f\"Format: Mean (95% CI)\")\n",
        "print(\"----------------------------------------------------------\")\n",
        "print(f\"Test F1-Score:   {f1_mean:.4f} (95% CI: [{f1_ci[0]:.4f}, {f1_ci[1]:.4f}])\")\n",
        "print(f\"Test Accuracy:   {acc_mean:.4f} (95% CI: [{acc_ci[0]:.4f}, {acc_ci[1]:.4f}])\")\n",
        "print(f\"Test Precision:  {prec_mean:.4f} (95% CI: [{prec_ci[0]:.4f}, {prec_ci[1]:.4f}])\")\n",
        "print(f\"Test Recall:     {rec_mean:.4f} (95% CI: [{rec_ci[0]:.4f}, {rec_ci[1]:.4f}])\")\n",
        "print(\"----------------------------------------------------------\\n\")\n",
        "\n",
        "results_file = os.path.join(Config.DRIVE_PATH, 'models', 'step1_gated_baseline_results.txt')\n",
        "with open(results_file, 'w') as f:\n",
        "    f.write(\"--- FINAL GATED MODEL TEST RESULTS ---\\n\\n\")\n",
        "    f.write(f\"Model: {Config.MODEL_NAME}\\n\\n\")\n",
        "\n",
        "    f.write(\"--- SINGLE PASS (CLEAN) RESULTS ---\\n\")\n",
        "    f.write(f\"Test F1-Score:   {clean_test_results['eval_f1']:.4f}\\n\")\n",
        "    f.write(f\"Test Accuracy:   {clean_test_results['eval_accuracy']:.4f}\\n\")\n",
        "    f.write(f\"Test Precision:  {clean_test_results['eval_precision']:.4f}\\n\")\n",
        "    f.write(f\"Test Recall:     {clean_test_results['eval_recall']:.4f}\\n\\n\")\n",
        "\n",
        "    f.write(f\"--- BOOTSTRAPPED RESULTS ({Config.N_BOOTSTRAPS} samples) ---\\n\")\n",
        "    f.write(f\"Format: Mean (95% CI)\\n\")\n",
        "    f.write(f\"Test F1-Score:   {f1_mean:.4f} (95% CI: [{f1_ci[0]:.4f}, {f1_ci[1]:.4f}])\\n\")\n",
        "    f.write(f\"Test Accuracy:   {acc_mean:.4f} (95% CI: [{acc_ci[0]:.4f}, {acc_ci[1]:.4f}])\\n\")\n",
        "    f.write(f\"Test Precision:  {prec_mean:.4f} (95% CI: [{prec_ci[0]:.4f}, {prec_ci[1]:.4f}])\\n\")\n",
        "    f.write(f\"Test Recall:     {rec_mean:.4f} (95% CI: [{rec_ci[0]:.4f}, {rec_ci[1]:.4f}])\\n\")\n",
        "\n",
        "print(f\"Test results saved to {results_file}\")\n",
        "print(\"--- Cell 13 Complete ---\")\n",
        "\n",
        "\n",
        "# --- Cell 14: Clean Up Memory ---\n",
        "print(\"Cleaning up memory...\")\n",
        "del model\n",
        "del trainer\n",
        "del tokenized_datasets\n",
        "del raw_datasets\n",
        "gc.collect()\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "print(\"--- STEP 1 COMPLETE ---\")\n",
        "print(\"You now have a trained, saved, and evaluated GATED model.\")\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "P7ta4-j3f7rb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SINGLE CELL: self-contained adversarial suite runner\n",
        "# 1) Set your paths here:\n",
        "DRIVE_BASE          = r\"/content/drive/MyDrive/hate\"\n",
        "TRAIN_FILE          = f\"{DRIVE_BASE}/train.csv\"\n",
        "VAL_FILE            = f\"{DRIVE_BASE}/val.csv\"\n",
        "TEST_FILE           = f\"{DRIVE_BASE}/test.csv\"\n",
        "BASELINE_CHECKPOINT = f\"{DRIVE_BASE}/models/step1_bert_baseline\"     # saved baseline model\n",
        "GATED_CHECKPOINT    = f\"{DRIVE_BASE}/models/step1_gated_fusion\"      # saved gated model (folder)\n",
        "\n",
        "# 2) Install deps\n",
        "!pip -q install nlpaug nltk transformers datasets -q\n",
        "\n",
        "# 3) Imports\n",
        "import os, re, json, random\n",
        "from copy import deepcopy\n",
        "from tqdm.auto import tqdm\n",
        "import numpy as np\n",
        "from datasets import Dataset, load_dataset\n",
        "import nlpaug.augmenter.char as nac\n",
        "from nlpaug.augmenter.word import SynonymAug\n",
        "import nltk\n",
        "nltk.download('wordnet', quiet=True)\n",
        "from nltk.corpus import wordnet as wn\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    AutoConfig,\n",
        "    AutoModel,\n",
        "    PreTrainedModel,\n",
        "    PretrainedConfig\n",
        ")\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# -----------------------\n",
        "# Gated model class + config\n",
        "# -----------------------\n",
        "class GFConfig(PretrainedConfig):\n",
        "    model_type = \"gated_fusion_wrapper\"\n",
        "    def __init__(self, base_model_name: str = \"bert-base-multilingual-cased\", num_labels: int = 2, gate_hidden: int = 256, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.base_model_name = base_model_name\n",
        "        self.num_labels = num_labels\n",
        "        self.gate_hidden = gate_hidden\n",
        "\n",
        "class GatedFusionForSequenceClassification(PreTrainedModel):\n",
        "    config_class = GFConfig\n",
        "    def __init__(self, config: GFConfig):\n",
        "        super().__init__(config)\n",
        "        self.base_cfg = AutoConfig.from_pretrained(config.base_model_name)\n",
        "        self.encoder = AutoModel.from_pretrained(config.base_model_name, config=self.base_cfg)\n",
        "        hidden = self.base_cfg.hidden_size\n",
        "        self.gate_mlp = nn.Sequential(\n",
        "            nn.Linear(2 * hidden, config.gate_hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(config.gate_hidden, hidden),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        self.dropout = nn.Dropout(getattr(self.base_cfg, \"hidden_dropout_prob\", 0.1))\n",
        "        self.classifier = nn.Linear(hidden, config.num_labels)\n",
        "        self.post_init()\n",
        "    @staticmethod\n",
        "    def masked_mean(last_hidden_state, attention_mask):\n",
        "        mask = attention_mask.unsqueeze(-1).type_as(last_hidden_state)\n",
        "        summed = (last_hidden_state * mask).sum(dim=1)\n",
        "        denom = mask.sum(dim=1).clamp(min=1e-6)\n",
        "        return summed / denom\n",
        "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, labels=None, **kwargs):\n",
        "        allowed = {\"position_ids\", \"head_mask\", \"inputs_embeds\", \"output_attentions\", \"output_hidden_states\", \"return_dict\", \"past_key_values\", \"encoder_hidden_states\", \"encoder_attention_mask\"}\n",
        "        safe_kwargs = {k: v for k, v in kwargs.items() if k in allowed}\n",
        "        safe_kwargs.pop(\"num_items_in_batch\", None)\n",
        "        enc = self.encoder(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, **safe_kwargs)\n",
        "        h_cls = enc.last_hidden_state[:, 0, :]\n",
        "        h_mean = self.masked_mean(enc.last_hidden_state, attention_mask)\n",
        "        gate_inp = torch.cat([h_cls, h_mean], dim=-1)\n",
        "        g = self.gate_mlp(gate_inp)\n",
        "        fused = g * h_cls + (1.0 - g) * h_mean\n",
        "        fused = self.dropout(fused)\n",
        "        logits = self.classifier(fused)\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(logits.view(-1, self.config.num_labels), labels.view(-1))\n",
        "        return {\"loss\": loss, \"logits\": logits}\n",
        "\n",
        "# -----------------------\n",
        "# Attack tools (working set)\n",
        "# -----------------------\n",
        "structural_typo   = nac.KeyboardAug()\n",
        "structural_insert = nac.RandomCharAug(action=\"insert\")\n",
        "\n",
        "def attack_structural_case(text, p_char=0.15):\n",
        "    if not isinstance(text, str):\n",
        "        return text\n",
        "    out = []\n",
        "    for ch in text:\n",
        "        if ch.isalpha() and random.random() < p_char:\n",
        "            out.append(ch.upper() if ch.islower() else ch.lower())\n",
        "        else:\n",
        "            out.append(ch)\n",
        "    return \"\".join(out)\n",
        "\n",
        "synonym_aug = SynonymAug(aug_src='wordnet', aug_p=0.15)\n",
        "\n",
        "def attack_structural_typo(text):\n",
        "    try:\n",
        "        return structural_typo.augment(text)[0]\n",
        "    except Exception:\n",
        "        return text\n",
        "\n",
        "def attack_structural_insert(text):\n",
        "    try:\n",
        "        return structural_insert.augment(text)[0]\n",
        "    except Exception:\n",
        "        return text\n",
        "\n",
        "def attack_semantic_synonym(text):\n",
        "    try:\n",
        "        out = synonym_aug.augment(text)\n",
        "        if isinstance(out, list) and len(out) > 0:\n",
        "            return out[0]\n",
        "        return out\n",
        "    except Exception:\n",
        "        if not isinstance(text, str): return text\n",
        "        toks = text.split()\n",
        "        idxs = list(range(len(toks))); random.shuffle(idxs)\n",
        "        for i in idxs:\n",
        "            tok = toks[i]\n",
        "            synsets = wn.synsets(tok)\n",
        "            if not synsets: continue\n",
        "            lemmas = []\n",
        "            for syn in synsets[:3]:\n",
        "                for l in syn.lemmas():\n",
        "                    cand = l.name().replace('_', ' ')\n",
        "                    if cand.lower() != tok.lower(): lemmas.append(cand)\n",
        "            if lemmas:\n",
        "                toks[i] = random.choice(lemmas); break\n",
        "        return \" \".join(toks)\n",
        "\n",
        "CODED_LEXICON = {\n",
        "    r'\\bmuslims?\\b': 'skittles',\n",
        "    r'\\bblack( people)?\\b': 'googles',\n",
        "    r'\\bjews?\\b': 'skypes',\n",
        "    r'\\bmexicans?\\b': 'bings',\n",
        "}\n",
        "CODED_PATTERNS = [(re.compile(k, flags=re.IGNORECASE), v) for k, v in CODED_LEXICON.items()]\n",
        "def attack_semantic_coded(text):\n",
        "    if not isinstance(text, str): return text\n",
        "    out = text\n",
        "    for pat, repl in CODED_PATTERNS:\n",
        "        out = pat.sub(repl, out)\n",
        "    return out\n",
        "\n",
        "SLUR_LEXICON = [\n",
        "    r\"\\bmongol(s)?\\b\", r\"\\bretard(s|ed)?\\b\", r\"\\btolol\\b\", r\"\\bkontol\\b\",\n",
        "    r\"\\bbajingan\\b\", r\"\\bbabi\\b\", r\"\\bbhen ?chod\\b\", r\"\\bmadar ?chod\\b\",\n",
        "    r\"\\brandi\\b\", r\"\\bperra\\b\", r\"\\bzorra\\b\", r\"\\bputa\\b\"\n",
        "]\n",
        "SLUR_PATTERNS = [re.compile(p, flags=re.IGNORECASE) for p in SLUR_LEXICON]\n",
        "def attack_feature_slur_removal(text, replacement='[removed]'):\n",
        "    if not isinstance(text, str): return text\n",
        "    out = text\n",
        "    for pat in SLUR_PATTERNS:\n",
        "        out = pat.sub(replacement, out)\n",
        "    return out\n",
        "\n",
        "def apply_attack_texts(texts, attack_fn, desc=\"Attacking\"):\n",
        "    return [attack_fn(t) for t in tqdm(texts, desc=desc, leave=False)]\n",
        "\n",
        "# -----------------------\n",
        "# Per-class metrics helper\n",
        "# -----------------------\n",
        "def _per_class_metrics(y_true, y_pred, labels=(0, 1)):\n",
        "    p, r, f1, sup = precision_recall_fscore_support(y_true, y_pred, labels=list(labels), zero_division=0)\n",
        "    return [\n",
        "        {\"label\": int(lbl), \"precision\": float(p[i]), \"recall\": float(r[i]), \"f1\": float(f1[i]), \"support\": int(sup[i])}\n",
        "        for i, lbl in enumerate(labels)\n",
        "    ]\n",
        "\n",
        "# -----------------------\n",
        "# Data bootstrap (auto-load if absent)\n",
        "# -----------------------\n",
        "def _ensure_data_loaded():\n",
        "    global raw_datasets, tokenized_datasets\n",
        "    need_raw = 'raw_datasets' not in globals()\n",
        "    need_tok = 'tokenized_datasets' not in globals()\n",
        "    if not (need_raw or need_tok):\n",
        "        return  # already present\n",
        "\n",
        "    # Load CSVs into a datasets.DatasetDict\n",
        "    data_files = {}\n",
        "    if os.path.isfile(TRAIN_FILE): data_files['train'] = TRAIN_FILE\n",
        "    if os.path.isfile(VAL_FILE):   data_files['validation'] = VAL_FILE\n",
        "    if os.path.isfile(TEST_FILE):  data_files['test'] = TEST_FILE\n",
        "    if not data_files:\n",
        "        raise RuntimeError(\"No data files found. Set TRAIN_FILE/VAL_FILE/TEST_FILE correctly.\")\n",
        "\n",
        "    raw_datasets = load_dataset(\"csv\", data_files=data_files)\n",
        "\n",
        "    # Choose a tokenizer: prefer baseline checkpoint tokenizer if present, else default multilingual BERT\n",
        "    tok_path = BASELINE_CHECKPOINT if os.path.isdir(BASELINE_CHECKPOINT) else \"bert-base-multilingual-cased\"\n",
        "    _tokenizer = AutoTokenizer.from_pretrained(tok_path)\n",
        "\n",
        "    def tokenize_batch(batch):\n",
        "        return _tokenizer(batch['text'], padding='max_length', truncation=True, max_length=128)\n",
        "\n",
        "    tokenized_datasets = raw_datasets.map(tokenize_batch, batched=True)\n",
        "    # rename label->labels for Trainer compatibility & set torch format\n",
        "    if 'label' in tokenized_datasets['test'].column_names:\n",
        "        tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
        "    tokenized_datasets.set_format(\"torch\")\n",
        "\n",
        "_ensure_data_loaded()\n",
        "\n",
        "# -----------------------\n",
        "# Eval helpers & suite\n",
        "# -----------------------\n",
        "def evaluate_on_texts(trainer, tokenizer, texts, labels, max_length=128):\n",
        "    encodings = tokenizer(texts, padding='max_length', truncation=True, max_length=max_length, return_tensors='pt')\n",
        "    ds = Dataset.from_dict({k: encodings[k].tolist() for k in encodings})\n",
        "    ds = ds.add_column(\"label\", labels)\n",
        "    ds.set_format(type='torch', columns=list(encodings.keys()) + [\"label\"])\n",
        "    preds_out = trainer.predict(ds)\n",
        "    logits = preds_out.predictions\n",
        "    preds = np.argmax(logits, axis=-1)\n",
        "    metrics = {}\n",
        "    return preds, logits, metrics\n",
        "\n",
        "def run_attack_and_report(trainer, tokenizer, raw_test_dataset, tokenized_test_dataset, attack_fn, attack_name, attack_mode='tp', save_dir=None):\n",
        "    preds_out = trainer.predict(tokenized_test_dataset)\n",
        "    clean_logits = preds_out.predictions\n",
        "    clean_preds = np.argmax(clean_logits, axis=-1)\n",
        "    clean_labels = preds_out.label_ids\n",
        "\n",
        "    # Per-class metrics (clean)\n",
        "    clean_per_class = _per_class_metrics(clean_labels, clean_preds)\n",
        "\n",
        "    # Extract raw texts & labels\n",
        "    raw_texts = list(raw_test_dataset['text'])\n",
        "    raw_labels = list(raw_test_dataset['label'])\n",
        "    n = len(raw_texts)\n",
        "    assert n == len(clean_preds) == len(clean_labels)\n",
        "\n",
        "    # choose indices\n",
        "    if attack_mode == 'tp':\n",
        "        target_indices = [i for i, (lab, pred) in enumerate(zip(clean_labels, clean_preds)) if lab == 1 and pred == 1]\n",
        "    elif attack_mode == 'full':\n",
        "        target_indices = list(range(n))\n",
        "    else:\n",
        "        raise ValueError(\"attack_mode must be 'tp' or 'full'\")\n",
        "\n",
        "    if len(target_indices) == 0:\n",
        "        report = {\n",
        "            'attack_name': attack_name, 'attack_mode': attack_mode,\n",
        "            'clean_metrics': {}, 'clean_per_class': clean_per_class,\n",
        "            'attacked_metrics': None, 'attacked_per_class': None,\n",
        "            'attack_changed': 0, 'tp_count_targeted': 0, 'note': 'no targets'\n",
        "        }\n",
        "        if save_dir:\n",
        "            os.makedirs(save_dir, exist_ok=True)\n",
        "            with open(os.path.join(save_dir, f\"{attack_name}_{attack_mode}.json\"), \"w\") as f: json.dump(report, f, indent=2)\n",
        "        return report\n",
        "\n",
        "    attacked_texts = raw_texts.copy()\n",
        "    to_attack = [raw_texts[i] for i in target_indices]\n",
        "    attacked_outs = apply_attack_texts(to_attack, attack_fn, desc=attack_name)\n",
        "    attack_count = 0\n",
        "    for idx, new_text in zip(target_indices, attacked_outs):\n",
        "        if new_text != raw_texts[idx]:\n",
        "            attacked_texts[idx] = new_text\n",
        "            attack_count += 1\n",
        "\n",
        "    if attack_count == 0:\n",
        "        report = {\n",
        "            'attack_name': attack_name, 'attack_mode': attack_mode,\n",
        "            'clean_metrics': {}, 'clean_per_class': clean_per_class,\n",
        "            'attacked_metrics': None, 'attacked_per_class': None,\n",
        "            'attack_changed': 0, 'tp_count_targeted': len(target_indices),\n",
        "            'note': 'no modification made by attack_fn'\n",
        "        }\n",
        "        if save_dir:\n",
        "            os.makedirs(save_dir, exist_ok=True)\n",
        "            with open(os.path.join(save_dir, f\"{attack_name}_{attack_mode}.json\"), \"w\") as f: json.dump(report, f, indent=2)\n",
        "        return report\n",
        "\n",
        "    # evaluate attacked set\n",
        "    attacked_preds, attacked_logits, attacked_metrics = evaluate_on_texts(trainer, tokenizer, attacked_texts, raw_labels)\n",
        "    attacked_per_class = _per_class_metrics(raw_labels, attacked_preds)\n",
        "\n",
        "    # For TP mode: ASR = % attacked TPs flipped to class 0\n",
        "    asr = None; samples_flipped = None\n",
        "    if attack_mode == 'tp':\n",
        "        attacked_for_targets_preds = [attacked_preds[i] for i in target_indices]\n",
        "        samples_flipped = sum(1 for p in attacked_for_targets_preds if p == 0)\n",
        "        asr = samples_flipped / len(target_indices)\n",
        "\n",
        "    # (Optional) delta F1 omitted since we're not recomputing macro-F1 here\n",
        "    report = {\n",
        "        'attack_name': attack_name,\n",
        "        'attack_mode': attack_mode,\n",
        "        'clean_per_class': clean_per_class,\n",
        "        'attacked_per_class': attacked_per_class,\n",
        "        'attack_changed': attack_count,\n",
        "        'tp_count_targeted': len(target_indices),\n",
        "        'samples_flipped': int(samples_flipped) if samples_flipped is not None else None,\n",
        "        'attack_success_rate': float(asr) if asr is not None else None\n",
        "    }\n",
        "    if save_dir:\n",
        "        os.makedirs(save_dir, exist_ok=True)\n",
        "        with open(os.path.join(save_dir, f\"{attack_name}_{attack_mode}.json\"), \"w\") as f: json.dump(report, f, indent=2)\n",
        "    return report\n",
        "\n",
        "def run_all_attacks(trainer, tokenizer, raw_test_dataset, tokenized_test_dataset, save_dir=\"attack_reports\", attack_mode='tp', attacks_to_run=None):\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    attacks = attacks_to_run or [\n",
        "        (attack_structural_typo, 'structural_typo'),\n",
        "        (attack_structural_insert, 'structural_insert'),\n",
        "        (attack_structural_case, 'structural_case'),\n",
        "        (attack_semantic_synonym, 'semantic_synonym'),\n",
        "        (attack_semantic_coded, 'semantic_coded'),\n",
        "        (lambda t: attack_feature_slur_removal(t, replacement='[removed]'), 'feature_slur_removal'),\n",
        "    ]\n",
        "    suite_report = {'model': str(trainer.model.__class__), 'attack_mode': attack_mode, 'attacks': []}\n",
        "    for fn, name in attacks:\n",
        "        print(f\"Running attack: {name} (mode={attack_mode})\")\n",
        "        rpt = run_attack_and_report(trainer, tokenizer, raw_test_dataset, tokenized_test_dataset, fn, name, attack_mode, save_dir=save_dir)\n",
        "        if rpt is not None:\n",
        "            suite_report['attacks'].append(rpt)\n",
        "    with open(os.path.join(save_dir, f\"suite_report_{trainer.model.__class__.__name__}_{attack_mode}.json\"), 'w') as f:\n",
        "        json.dump(suite_report, f, indent=2)\n",
        "    return suite_report\n",
        "\n",
        "# -----------------------\n",
        "# Build eval-only trainers (auto; gated falls back to custom if needed)\n",
        "# -----------------------\n",
        "eval_args = TrainingArguments(\n",
        "    output_dir=\"./tmp_eval\",\n",
        "    per_device_eval_batch_size=32,\n",
        "    do_train=False, do_eval=True,\n",
        "    report_to=\"none\", logging_strategy=\"no\",\n",
        "    disable_tqdm=True\n",
        ")\n",
        "\n",
        "def build_eval_trainer(checkpoint_path, tokenizer=None):\n",
        "    if not os.path.isdir(checkpoint_path):\n",
        "        raise FileNotFoundError(checkpoint_path)\n",
        "    tok = tokenizer or AutoTokenizer.from_pretrained(checkpoint_path)\n",
        "    try:\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(checkpoint_path)\n",
        "        model.to(device)\n",
        "        return Trainer(model=model, args=eval_args, tokenizer=tok, compute_metrics=None)\n",
        "    except Exception:\n",
        "        # Try gated fallback\n",
        "        cfg = GFConfig.from_pretrained(checkpoint_path)\n",
        "        model = GatedFusionForSequenceClassification(cfg)\n",
        "        state_path = os.path.join(checkpoint_path, \"pytorch_model.bin\")\n",
        "        if os.path.isfile(state_path):\n",
        "            state_dict = torch.load(state_path, map_location=device)\n",
        "            model.load_state_dict(state_dict, strict=False)\n",
        "        model.to(device)\n",
        "        return Trainer(model=model, args=eval_args, tokenizer=tok, compute_metrics=None)\n",
        "\n",
        "# -----------------------\n",
        "# Run: baseline then gated\n",
        "# -----------------------\n",
        "# Tokenizers (one per checkpoint, so embeddings/normalization match)\n",
        "baseline_tok = AutoTokenizer.from_pretrained(BASELINE_CHECKPOINT if os.path.isdir(BASELINE_CHECKPOINT) else \"bert-base-multilingual-cased\")\n",
        "gated_tok    = AutoTokenizer.from_pretrained(GATED_CHECKPOINT    if os.path.isdir(GATED_CHECKPOINT)    else \"bert-base-multilingual-cased\")\n",
        "\n",
        "print(\"Building baseline trainer...\")\n",
        "trainer_baseline = build_eval_trainer(BASELINE_CHECKPOINT, tokenizer=baseline_tok)\n",
        "print(\"Baseline trainer ready. Running attacks...\")\n",
        "baseline_report = run_all_attacks(\n",
        "    trainer_baseline,\n",
        "    tokenizer=baseline_tok,\n",
        "    raw_test_dataset=raw_datasets['test'],\n",
        "    tokenized_test_dataset=tokenized_datasets['test'],\n",
        "    save_dir=\"attack_reports/baseline\",\n",
        "    attack_mode='tp'\n",
        ")\n",
        "print(\"Baseline attacks finished. Reports -> attack_reports/baseline\")\n",
        "\n",
        "print(\"\\nBuilding gated trainer...\")\n",
        "trainer_gated = build_eval_trainer(GATED_CHECKPOINT, tokenizer=gated_tok)\n",
        "print(\"Gated trainer ready. Running attacks...\")\n",
        "gated_report = run_all_attacks(\n",
        "    trainer_gated,\n",
        "    tokenizer=gated_tok,\n",
        "    raw_test_dataset=raw_datasets['test'],\n",
        "    tokenized_test_dataset=tokenized_datasets['test'],\n",
        "    save_dir=\"attack_reports/gated\",\n",
        "    attack_mode='tp'\n",
        ")\n",
        "print(\"Gated attacks finished. Reports -> attack_reports/gated\")\n",
        "\n",
        "print(\"\\nALL DONE   Check JSONs under attack_reports/ (each includes per-class metrics).\")\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "-mKuSQhpJzkw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================\n",
        "# Sentinel Architecture + Robust Training (FGM + Consistency)\n",
        "# ===========================\n",
        "!pip -q install transformers datasets scikit-learn\n",
        "\n",
        "import os, math, re, json, random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from dataclasses import dataclass\n",
        "from typing import Optional, Dict, Any, List\n",
        "\n",
        "from datasets import load_dataset, DatasetDict\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoModel, AutoConfig,\n",
        "    PretrainedConfig, PreTrainedModel,\n",
        "    Trainer, TrainingArguments\n",
        ")\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# -------------------------\n",
        "# Config (EDIT THESE PATHS)\n",
        "# -------------------------\n",
        "class Config:\n",
        "    DRIVE = \"/content/drive/MyDrive/hate\"\n",
        "    TRAIN = os.path.join(DRIVE, \"train.csv\")\n",
        "    VAL   = os.path.join(DRIVE, \"val.csv\")\n",
        "    TEST  = os.path.join(DRIVE, \"test.csv\")\n",
        "\n",
        "    BASE_MODEL = \"xlm-roberta-base\"  # multilingual & strong; swap to roberta-large for EN-only\n",
        "    MAX_LEN    = 128\n",
        "    BATCH      = 16\n",
        "    EPOCHS     = 5\n",
        "    LR         = 2e-5\n",
        "    NUM_LABELS = 2\n",
        "\n",
        "    # Sentinel toggles\n",
        "    USE_SENTINEL       = True     # set False to run plain encoder classifier (baseline)\n",
        "    HEURISTIC_DIM      = 32\n",
        "    HEURISTIC_HIDDEN   = 256\n",
        "    CAUSAL_HIDDEN      = 256\n",
        "    ATTENTION_HEADS    = 8\n",
        "    AUX_CASUAL_LOSS_W  = 0.2\n",
        "\n",
        "    # Robustness additions\n",
        "    ALPHA_ADV       = 0.5     # weight for adversarial loss (FGM)\n",
        "    BETA_CONS       = 0.2     # weight for consistency loss\n",
        "    FGM_EPS         = 1e-3    # magnitude of FGM perturbation on embeddings\n",
        "    CONS_TOK_MASK_P = 0.08    # probability to mask tokens for consistency\n",
        "\n",
        "    SAVE_DIR = os.path.join(DRIVE, \"models\", \"sentinel_xlmr\")\n",
        "\n",
        "# -------------------------\n",
        "# Heuristic feature builder\n",
        "# -------------------------\n",
        "SLUR_REGEXES = [\n",
        "    r\"\\bmongol(s)?\\b\", r\"\\bretard(s|ed)?\\b\", r\"\\btolol\\b\", r\"\\bkontol\\b\",\n",
        "    r\"\\bbajingan\\b\", r\"\\bbabi\\b\", r\"\\bbhen ?chod\\b\", r\"\\bmadar ?chod\\b\",\n",
        "    r\"\\brandi\\b\", r\"\\bperra\\b\", r\"\\bzorra\\b\", r\"\\bputa\\b\"\n",
        "]\n",
        "SLUR_PATTERNS = [re.compile(p, re.IGNORECASE) for p in SLUR_REGEXES]\n",
        "\n",
        "def build_heuristic_features(text: str) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Minimal, fast features; replace/extend with LIWC, sentiment, dependency, etc.\n",
        "    Size must equal Config.HEURISTIC_DIM (we'll pad/truncate).\n",
        "    \"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        text = \"\" if text is None else str(text)\n",
        "\n",
        "    length = len(text)\n",
        "    words  = text.split()\n",
        "    n_words = max(1, len(words))\n",
        "\n",
        "    upper = sum(1 for c in text if c.isalpha() and c.isupper())\n",
        "    digits = sum(1 for c in text if c.isdigit())\n",
        "    punct = sum(1 for c in text if c in \".,;:!?\")\n",
        "\n",
        "    # crude ratios\n",
        "    upper_ratio = upper / max(1, sum(c.isalpha() for c in text))\n",
        "    digit_ratio = digits / max(1, len(text))\n",
        "    punct_ratio = punct / max(1, len(text))\n",
        "\n",
        "    # slur density\n",
        "    slur_hits = 0\n",
        "    for pat in SLUR_PATTERNS:\n",
        "        slur_hits += len(pat.findall(text))\n",
        "    slur_density = slur_hits / n_words\n",
        "\n",
        "    # simplistic aggression cue count\n",
        "    cues = sum(text.lower().count(k) for k in [\"kill\", \"die\", \"trash\", \"dirty\", \"dog\", \"pig\", \"scum\", \"hate\"])\n",
        "    cue_density = cues / n_words\n",
        "\n",
        "    base_feats = np.array([\n",
        "        length, n_words, upper, digits, punct,\n",
        "        upper_ratio, digit_ratio, punct_ratio,\n",
        "        slur_hits, slur_density, cues, cue_density\n",
        "    ], dtype=np.float32)\n",
        "\n",
        "    # Normalize some scale-sensitive feats (very rough)\n",
        "    base_feats[0] = math.log1p(base_feats[0])   # length\n",
        "    base_feats[1] = math.log1p(base_feats[1])   # n_words\n",
        "\n",
        "    # Pad/truncate to HEURISTIC_DIM\n",
        "    H = Config.HEURISTIC_DIM\n",
        "    if base_feats.shape[0] < H:\n",
        "        pad = np.zeros(H - base_feats.shape[0], dtype=np.float32)\n",
        "        feats = np.concatenate([base_feats, pad])\n",
        "    else:\n",
        "        feats = base_feats[:H]\n",
        "    return feats\n",
        "\n",
        "# -------------------------\n",
        "# Dataset + Tokenization\n",
        "# -------------------------\n",
        "assert os.path.isfile(Config.TRAIN) and os.path.isfile(Config.VAL) and os.path.isfile(Config.TEST), \\\n",
        "    \"Train/Val/Test CSVs not found. Update Config paths.\"\n",
        "\n",
        "data_files = {\"train\": Config.TRAIN, \"validation\": Config.VAL, \"test\": Config.TEST}\n",
        "raw = load_dataset(\"csv\", data_files=data_files)\n",
        "tok = AutoTokenizer.from_pretrained(Config.BASE_MODEL)\n",
        "\n",
        "def tok_map(batch):\n",
        "    enc = tok(batch[\"text\"], padding=\"max_length\", truncation=True, max_length=Config.MAX_LEN)\n",
        "    # Heuristic features per example\n",
        "    feats = [build_heuristic_features(t) for t in batch[\"text\"]]\n",
        "    enc[\"heuristic_feats\"] = feats\n",
        "    # Optional auxiliary causal targets:\n",
        "    if \"causal_target\" in batch:\n",
        "        enc[\"causal_target\"] = batch[\"causal_target\"]\n",
        "    return enc\n",
        "\n",
        "tokenized = raw.map(tok_map, batched=True, remove_columns=[c for c in raw[\"train\"].column_names if c not in (\"text\",\"label\",\"causal_target\")])\n",
        "tokenized = tokenized.rename_column(\"label\", \"labels\")\n",
        "tokenized.set_format(type=\"torch\", columns=[\"input_ids\",\"attention_mask\",\"labels\",\"heuristic_feats\"] + ([\"causal_target\"] if \"causal_target\" in tokenized[\"train\"].column_names else []))\n",
        "\n",
        "# -------------------------\n",
        "# Data collator (to tensorize heuristic feats)\n",
        "# -------------------------\n",
        "@dataclass\n",
        "class SentinelCollator:\n",
        "    def __call__(self, batch: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
        "        out = {}\n",
        "        for k in (\"input_ids\", \"attention_mask\", \"labels\"):\n",
        "            out[k] = torch.stack([b[k] for b in batch])\n",
        "        feats = [torch.tensor(b[\"heuristic_feats\"], dtype=torch.float32) if not isinstance(b[\"heuristic_feats\"], torch.Tensor)\n",
        "                 else b[\"heuristic_feats\"].to(torch.float32)\n",
        "                 for b in batch]\n",
        "        out[\"heuristic_feats\"] = torch.stack(feats)\n",
        "        if \"causal_target\" in batch[0]:\n",
        "            out[\"causal_target\"] = torch.stack([b[\"causal_target\"] for b in batch]).long()\n",
        "        return out\n",
        "\n",
        "collator = SentinelCollator()\n",
        "\n",
        "# -------------------------\n",
        "# Sentinel Config + Model\n",
        "# -------------------------\n",
        "class SentinelConfig(PretrainedConfig):\n",
        "    model_type = \"sentinel_fusion\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        base_model_name: str = \"xlm-roberta-base\",\n",
        "        num_labels: int = 2,\n",
        "        heuristic_dim: int = 32,\n",
        "        heuristic_hidden: int = 256,\n",
        "        causal_hidden: int = 256,\n",
        "        attn_heads: int = 8,\n",
        "        aux_causal_loss_weight: float = 0.2,\n",
        "        **kwargs\n",
        "    ):\n",
        "        super().__init__(**kwargs)\n",
        "        self.base_model_name = base_model_name\n",
        "        self.num_labels = num_labels\n",
        "        self.heuristic_dim = heuristic_dim\n",
        "        self.heuristic_hidden = heuristic_hidden\n",
        "        self.causal_hidden = causal_hidden\n",
        "        self.attn_heads = attn_heads\n",
        "        self.aux_causal_loss_weight = aux_causal_loss_weight\n",
        "\n",
        "class SentinelModel(PreTrainedModel):\n",
        "    config_class = SentinelConfig\n",
        "    def __init__(self, config: SentinelConfig):\n",
        "        super().__init__(config)\n",
        "        self.base_cfg = AutoConfig.from_pretrained(config.base_model_name)\n",
        "        self.encoder = AutoModel.from_pretrained(config.base_model_name, config=self.base_cfg)\n",
        "        hidden = self.base_cfg.hidden_size\n",
        "\n",
        "        # Heuristic projector\n",
        "        self.heuristic_proj = nn.Sequential(\n",
        "            nn.Linear(config.heuristic_dim, config.heuristic_hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(config.heuristic_hidden, hidden),\n",
        "            nn.LayerNorm(hidden)\n",
        "        )\n",
        "\n",
        "        # Causal pathway\n",
        "        self.causal_mlp = nn.Sequential(\n",
        "            nn.Linear(hidden, config.causal_hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(config.causal_hidden, hidden),\n",
        "            nn.LayerNorm(hidden)\n",
        "        )\n",
        "        self.causal_aux_head = nn.Linear(hidden, 2)\n",
        "\n",
        "        # Cross-attention: Query = [CLS], KV = [heuristic, causal]\n",
        "        self.xattn = nn.MultiheadAttention(embed_dim=hidden, num_heads=config.attn_heads, batch_first=True)\n",
        "\n",
        "        self.dropout = nn.Dropout(getattr(self.base_cfg, \"hidden_dropout_prob\", 0.1))\n",
        "        self.classifier = nn.Linear(hidden, config.num_labels)\n",
        "        self.post_init()\n",
        "\n",
        "    @staticmethod\n",
        "    def masked_mean(last_hidden_state, attention_mask):\n",
        "        mask = attention_mask.unsqueeze(-1).type_as(last_hidden_state)\n",
        "        summed = (last_hidden_state * mask).sum(dim=1)\n",
        "        denom = mask.sum(dim=1).clamp(min=1e-6)\n",
        "        return summed / denom\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        heuristic_feats=None,\n",
        "        labels=None,\n",
        "        causal_target: Optional[torch.Tensor] = None,\n",
        "        **kwargs\n",
        "    ):\n",
        "        enc = self.encoder(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
        "        last_hidden = enc.last_hidden_state                  # [B, L, H]\n",
        "        h_cls = last_hidden[:, 0, :]                         # [B, H] (semantic query)\n",
        "\n",
        "        h_heu = self.heuristic_proj(heuristic_feats)         # [B, H]\n",
        "        h_cau = self.causal_mlp(h_cls)                       # [B, H]\n",
        "\n",
        "        Q = h_cls.unsqueeze(1)                               # [B, 1, H]\n",
        "        KV = torch.stack([h_heu, h_cau], dim=1)              # [B, 2, H]\n",
        "        fused, _ = self.xattn(Q, KV, KV)                     # [B, 1, H]\n",
        "        fused = fused.squeeze(1)                             # [B, H]\n",
        "        fused = self.dropout(fused)\n",
        "        logits = self.classifier(fused)                      # [B, C]\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss = nn.CrossEntropyLoss()(logits, labels)\n",
        "\n",
        "        if causal_target is not None:\n",
        "            cau_logits = self.causal_aux_head(h_cau)         # [B, 2]\n",
        "            aux_loss = nn.CrossEntropyLoss()(cau_logits, causal_target)\n",
        "            if loss is None:\n",
        "                loss = self.config.aux_causal_loss_weight * aux_loss\n",
        "            else:\n",
        "                loss = loss + self.config.aux_causal_loss_weight * aux_loss\n",
        "\n",
        "        return {\"loss\": loss, \"logits\": logits}\n",
        "\n",
        "# -------------------------\n",
        "# Baseline Model (no fusion)\n",
        "# -------------------------\n",
        "class BaselineClassifier(PreTrainedModel):\n",
        "    config_class = SentinelConfig\n",
        "    def __init__(self, config: SentinelConfig):\n",
        "        super().__init__(config)\n",
        "        self.base_cfg = AutoConfig.from_pretrained(config.base_model_name)\n",
        "        self.encoder = AutoModel.from_pretrained(config.base_model_name, config=self.base_cfg)\n",
        "        hidden = self.base_cfg.hidden_size\n",
        "        self.dropout = nn.Dropout(getattr(self.base_cfg, \"hidden_dropout_prob\", 0.1))\n",
        "        self.classifier = nn.Linear(hidden, config.num_labels)\n",
        "        self.post_init()\n",
        "\n",
        "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, labels=None, **kwargs):\n",
        "        enc = self.encoder(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
        "        h_cls = enc.last_hidden_state[:, 0, :]\n",
        "        logits = self.classifier(self.dropout(h_cls))\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss = nn.CrossEntropyLoss()(logits, labels)\n",
        "        return {\"loss\": loss, \"logits\": logits}\n",
        "\n",
        "# -------------------------\n",
        "# Metrics (macro)\n",
        "# -------------------------\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    preds = np.argmax(logits, axis=-1)\n",
        "    return {\n",
        "        \"accuracy\":  accuracy_score(labels, preds),\n",
        "        \"precision\": precision_score(labels, preds, average=\"macro\", zero_division=0),\n",
        "        \"recall\":    recall_score(labels, preds, average=\"macro\", zero_division=0),\n",
        "        \"f1\":        f1_score(labels, preds, average=\"macro\", zero_division=0),\n",
        "    }\n",
        "\n",
        "# -------------------------\n",
        "# Robustness additions\n",
        "# -------------------------\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def symmetric_kl(logits_p, logits_q, temperature=1.0):\n",
        "    p = F.log_softmax(logits_p/temperature, dim=-1)\n",
        "    q = F.log_softmax(logits_q/temperature, dim=-1)\n",
        "    p_soft = p.exp()\n",
        "    q_soft = q.exp()\n",
        "    return 0.5 * (F.kl_div(p, q_soft, reduction='batchmean') +\n",
        "                  F.kl_div(q, p_soft, reduction='batchmean'))\n",
        "\n",
        "@torch.no_grad()\n",
        "def corrupt_inputs_for_consistency(input_ids, attention_mask, mask_token_id, p=0.08):\n",
        "    x = input_ids.clone()\n",
        "    B, L = x.size()\n",
        "    rand = torch.rand_like(x.float())\n",
        "    corrupt_mask = (attention_mask == 1) & (rand < p)\n",
        "    corrupt_mask[:, 0] = False  # keep CLS intact\n",
        "    x[corrupt_mask] = mask_token_id\n",
        "    return x\n",
        "\n",
        "class FGM:\n",
        "    def __init__(self, model, epsilon=1e-3):\n",
        "        self.model = model\n",
        "        self.epsilon = epsilon\n",
        "        self.backup = None\n",
        "    def _emb(self):\n",
        "        return self.model.encoder.get_input_embeddings().weight\n",
        "    def attack(self):\n",
        "        emb = self._emb()\n",
        "        if emb.grad is None:\n",
        "            return False\n",
        "        grad = emb.grad\n",
        "        norm = torch.norm(grad)\n",
        "        if torch.isnan(norm) or torch.isinf(norm) or norm.item() == 0:\n",
        "            return False\n",
        "        self.backup = emb.data.clone()\n",
        "        r_adv = self.epsilon * grad / (norm + 1e-12)\n",
        "        emb.data.add_(r_adv)\n",
        "        return True\n",
        "    def restore(self):\n",
        "        if self.backup is not None:\n",
        "            self._emb().data = self.backup\n",
        "            self.backup = None\n",
        "\n",
        "from transformers import Trainer\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class RobustTrainer(Trainer):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.fgm = FGM(self.model, epsilon=getattr(Config, \"FGM_EPS\", 1e-3))\n",
        "        self.alpha = getattr(Config, \"ALPHA_ADV\", 0.0)\n",
        "        self.beta  = getattr(Config, \"BETA_CONS\", 0.0)\n",
        "        # resolve mask token id\n",
        "        self.mask_token_id = None\n",
        "        if getattr(self, \"processing_class\", None) is not None and getattr(self.processing_class, \"mask_token_id\", None) is not None:\n",
        "            self.mask_token_id = self.processing_class.mask_token_id\n",
        "        elif getattr(self, \"tokenizer\", None) is not None and getattr(self.tokenizer, \"mask_token_id\", None) is not None:\n",
        "            self.mask_token_id = self.tokenizer.mask_token_id\n",
        "        else:\n",
        "            try:\n",
        "                self.mask_token_id = self.model.encoder.config.mask_token_id\n",
        "            except Exception:\n",
        "                self.mask_token_id = None\n",
        "\n",
        "    # NOTE: accept the new arg `num_items_in_batch`\n",
        "    def training_step(self, model, inputs, num_items_in_batch=None):\n",
        "        model.train()\n",
        "        inputs = self._prepare_inputs(inputs)\n",
        "\n",
        "        # ---- main forward (CE  aux) ----\n",
        "        outputs = model(**inputs)\n",
        "        loss_main = outputs[\"loss\"] if isinstance(outputs, dict) else outputs[0]\n",
        "        total_loss = loss_main\n",
        "\n",
        "        # ---- consistency loss (noisy masked inputs) ----\n",
        "        if self.beta > 0 and self.mask_token_id is not None and \"input_ids\" in inputs and \"attention_mask\" in inputs:\n",
        "            with torch.no_grad():\n",
        "                noisy_inputs = {k: v for k, v in inputs.items()}\n",
        "                noisy_inputs[\"input_ids\"] = corrupt_inputs_for_consistency(\n",
        "                    inputs[\"input_ids\"], inputs[\"attention_mask\"],\n",
        "                    mask_token_id=self.mask_token_id,\n",
        "                    p=getattr(Config, \"CONS_TOK_MASK_P\", 0.08)\n",
        "                )\n",
        "            logits_clean = outputs[\"logits\"] if isinstance(outputs, dict) else outputs[1]\n",
        "            outputs_noisy = model(**noisy_inputs)\n",
        "            logits_noisy = outputs_noisy[\"logits\"] if isinstance(outputs_noisy, dict) else outputs_noisy[1]\n",
        "            loss_cons = symmetric_kl(logits_clean.detach(), logits_noisy)\n",
        "            total_loss = total_loss + self.beta * loss_cons\n",
        "\n",
        "        # backprop main+consistency\n",
        "        total_loss.backward()\n",
        "\n",
        "        # ---- FGM adversarial step ----\n",
        "        if self.alpha > 0:\n",
        "            if self.fgm.attack():\n",
        "                adv_outputs = model(**inputs)\n",
        "                loss_adv = adv_outputs[\"loss\"] if isinstance(adv_outputs, dict) else adv_outputs[0]\n",
        "                (self.alpha * loss_adv).backward()\n",
        "                self.fgm.restore()\n",
        "\n",
        "        self.optimizer.step()\n",
        "        self.lr_scheduler.step()\n",
        "        self.optimizer.zero_grad()\n",
        "        return total_loss.detach()\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Build model + trainer\n",
        "# -------------------------\n",
        "cfg = SentinelConfig(\n",
        "    base_model_name=Config.BASE_MODEL,\n",
        "    num_labels=Config.NUM_LABELS,\n",
        "    heuristic_dim=Config.HEURISTIC_DIM,\n",
        "    heuristic_hidden=Config.HEURISTIC_HIDDEN,\n",
        "    causal_hidden=Config.CAUSAL_HIDDEN,\n",
        "    attn_heads=Config.ATTENTION_HEADS,\n",
        "    aux_causal_loss_weight=Config.AUX_CASUAL_LOSS_W\n",
        ")\n",
        "\n",
        "model = (SentinelModel(cfg) if Config.USE_SENTINEL else BaselineClassifier(cfg)).to(device)\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir=Config.SAVE_DIR,\n",
        "    num_train_epochs=Config.EPOCHS,\n",
        "    learning_rate=Config.LR,\n",
        "    per_device_train_batch_size=Config.BATCH,\n",
        "    per_device_eval_batch_size=Config.BATCH * 2,\n",
        "    weight_decay=0.01,\n",
        "    warmup_ratio=0.06,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1\",\n",
        "    greater_is_better=True,\n",
        "    report_to=\"none\",\n",
        "    logging_strategy=\"no\",\n",
        "    disable_tqdm=False\n",
        ")\n",
        "\n",
        "trainer = RobustTrainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    data_collator=collator,\n",
        "    train_dataset=tokenized[\"train\"],\n",
        "    eval_dataset=tokenized[\"validation\"],\n",
        "    processing_class=tok,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "# -------------------------\n",
        "# Train & Evaluate\n",
        "# -------------------------\n",
        "print(f\"Training {'Sentinel' if Config.USE_SENTINEL else 'Baseline'} model with robustness losses...\")\n",
        "trainer.train()\n",
        "print(\"Evaluating on test...\")\n",
        "test_metrics = trainer.evaluate(tokenized[\"test\"])\n",
        "print({k: round(v, 4) for k, v in test_metrics.items()})\n",
        "\n",
        "# Save final model\n",
        "trainer.save_model(Config.SAVE_DIR)\n",
        "print(f\"Saved to: {Config.SAVE_DIR}\")\n"
      ],
      "metadata": {
        "id": "Oc6QLH6raoS8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === SINGLE CELL: Sentinel-only Adversarial Suite (with per-class metrics) ===\n",
        "# 1) Paths (edit as needed)\n",
        "DRIVE_BASE           = \"/content/drive/MyDrive/hate\"\n",
        "TRAIN_FILE           = f\"{DRIVE_BASE}/train.csv\"\n",
        "VAL_FILE             = f\"{DRIVE_BASE}/val.csv\"\n",
        "TEST_FILE            = f\"{DRIVE_BASE}/test.csv\"\n",
        "SENTINEL_CHECKPOINT  = f\"{DRIVE_BASE}/models/sentinel_xlmr\"  # <--- set to your saved Sentinel folder\n",
        "\n",
        "# 2) Deps\n",
        "!pip -q install nlpaug nltk transformers datasets scikit-learn\n",
        "\n",
        "# 3) Imports\n",
        "import os, re, json, math, random\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "import nlpaug.augmenter.char as nac\n",
        "from nlpaug.augmenter.word import SynonymAug\n",
        "import nltk\n",
        "nltk.download('wordnet', quiet=True)\n",
        "from nltk.corpus import wordnet as wn\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, Any, List, Optional\n",
        "from datasets import Dataset, load_dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoConfig, AutoModel,\n",
        "    PretrainedConfig, PreTrainedModel,\n",
        "    TrainingArguments, Trainer\n",
        ")\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# -------------------------\n",
        "# Sentinel config (must match training)\n",
        "# -------------------------\n",
        "class SentinelConfig(PretrainedConfig):\n",
        "    model_type = \"sentinel_fusion\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        base_model_name: str = \"xlm-roberta-base\",\n",
        "        num_labels: int = 2,\n",
        "        heuristic_dim: int = 32,         # <--- MUST match training\n",
        "        heuristic_hidden: int = 256,\n",
        "        causal_hidden: int = 256,\n",
        "        attn_heads: int = 8,\n",
        "        aux_causal_loss_weight: float = 0.2,\n",
        "        **kwargs\n",
        "    ):\n",
        "        super().__init__(**kwargs)\n",
        "        self.base_model_name = base_model_name\n",
        "        self.num_labels = num_labels\n",
        "        self.heuristic_dim = heuristic_dim\n",
        "        self.heuristic_hidden = heuristic_hidden\n",
        "        self.causal_hidden = causal_hidden\n",
        "        self.attn_heads = attn_heads\n",
        "        self.aux_causal_loss_weight = aux_causal_loss_weight\n",
        "\n",
        "# -------------------------\n",
        "# Sentinel model (same as training-time)\n",
        "# -------------------------\n",
        "class SentinelModel(PreTrainedModel):\n",
        "    config_class = SentinelConfig\n",
        "    def __init__(self, config: SentinelConfig):\n",
        "        super().__init__(config)\n",
        "        self.base_cfg = AutoConfig.from_pretrained(config.base_model_name)\n",
        "        self.encoder = AutoModel.from_pretrained(config.base_model_name, config=self.base_cfg)\n",
        "        hidden = self.base_cfg.hidden_size\n",
        "\n",
        "        # Heuristic projector\n",
        "        self.heuristic_proj = nn.Sequential(\n",
        "            nn.Linear(config.heuristic_dim, config.heuristic_hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(config.heuristic_hidden, hidden),\n",
        "            nn.LayerNorm(hidden)\n",
        "        )\n",
        "\n",
        "        # Causal pathway (from CLS)\n",
        "        self.causal_mlp = nn.Sequential(\n",
        "            nn.Linear(hidden, config.causal_hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(config.causal_hidden, hidden),\n",
        "            nn.LayerNorm(hidden)\n",
        "        )\n",
        "        self.causal_aux_head = nn.Linear(hidden, 2)  # (unused in eval)\n",
        "\n",
        "        # Cross-attention: Q = CLS; K,V = [heuristic, causal]\n",
        "        self.xattn = nn.MultiheadAttention(embed_dim=hidden, num_heads=config.attn_heads, batch_first=True)\n",
        "\n",
        "        self.dropout = nn.Dropout(getattr(self.base_cfg, \"hidden_dropout_prob\", 0.1))\n",
        "        self.classifier = nn.Linear(hidden, config.num_labels)\n",
        "        self.post_init()\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        heuristic_feats=None,\n",
        "        labels=None,\n",
        "        causal_target: Optional[torch.Tensor] = None,\n",
        "        **kwargs\n",
        "    ):\n",
        "        enc = self.encoder(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
        "        last_hidden = enc.last_hidden_state              # [B, L, H]\n",
        "        h_cls = last_hidden[:, 0, :]                     # [B, H]\n",
        "        h_heu = self.heuristic_proj(heuristic_feats)     # [B, H]\n",
        "        h_cau = self.causal_mlp(h_cls)                   # [B, H]\n",
        "\n",
        "        Q = h_cls.unsqueeze(1)                           # [B, 1, H]\n",
        "        KV = torch.stack([h_heu, h_cau], dim=1)          # [B, 2, H]\n",
        "        fused, _ = self.xattn(Q, KV, KV)                 # [B, 1, H]\n",
        "        fused = fused.squeeze(1)\n",
        "        logits = self.classifier(self.dropout(fused))\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss = nn.CrossEntropyLoss()(logits, labels)\n",
        "        return {\"loss\": loss, \"logits\": logits}\n",
        "\n",
        "# -------------------------\n",
        "# Heuristic features (must match training logic & HEURISTIC_DIM)\n",
        "# -------------------------\n",
        "SLUR_REGEXES = [\n",
        "    r\"\\bmongol(s)?\\b\", r\"\\bretard(s|ed)?\\b\", r\"\\btolol\\b\", r\"\\bkontol\\b\",\n",
        "    r\"\\bbajingan\\b\", r\"\\bbabi\\b\", r\"\\bbhen ?chod\\b\", r\"\\bmadar ?chod\\b\",\n",
        "    r\"\\brandi\\b\", r\"\\bperra\\b\", r\"\\bzorra\\b\", r\"\\bputa\\b\"\n",
        "]\n",
        "SLUR_PATTERNS = [re.compile(p, re.IGNORECASE) for p in SLUR_REGEXES]\n",
        "\n",
        "HEURISTIC_DIM = 32  # <--- set to the same value you trained with\n",
        "\n",
        "def build_heuristic_features(text: str) -> np.ndarray:\n",
        "    if not isinstance(text, str):\n",
        "        text = \"\" if text is None else str(text)\n",
        "\n",
        "    length = len(text)\n",
        "    words  = text.split()\n",
        "    n_words = max(1, len(words))\n",
        "\n",
        "    upper  = sum(1 for c in text if c.isalpha() and c.isupper())\n",
        "    digits = sum(1 for c in text if c.isdigit())\n",
        "    punct  = sum(1 for c in text if c in \".,;:!?\")\n",
        "\n",
        "    alpha_count = max(1, sum(c.isalpha() for c in text))\n",
        "    upper_ratio = upper / alpha_count\n",
        "    digit_ratio = digits / max(1, len(text))\n",
        "    punct_ratio = punct / max(1, len(text))\n",
        "\n",
        "    slur_hits = 0\n",
        "    for pat in SLUR_PATTERNS:\n",
        "        slur_hits += len(pat.findall(text))\n",
        "    slur_density = slur_hits / n_words\n",
        "\n",
        "    cues = sum(text.lower().count(k) for k in [\"kill\", \"die\", \"trash\", \"dirty\", \"dog\", \"pig\", \"scum\", \"hate\"])\n",
        "    cue_density = cues / n_words\n",
        "\n",
        "    base_feats = np.array([\n",
        "        math.log1p(length), math.log1p(n_words), upper, digits, punct,\n",
        "        upper_ratio, digit_ratio, punct_ratio,\n",
        "        slur_hits, slur_density, cues, cue_density\n",
        "    ], dtype=np.float32)\n",
        "\n",
        "    if base_feats.shape[0] < HEURISTIC_DIM:\n",
        "        pad = np.zeros(HEURISTIC_DIM - base_feats.shape[0], dtype=np.float32)\n",
        "        feats = np.concatenate([base_feats, pad])\n",
        "    else:\n",
        "        feats = base_feats[:HEURISTIC_DIM]\n",
        "    return feats\n",
        "\n",
        "# -------------------------\n",
        "# Data collator (stacks heuristic feats)\n",
        "# -------------------------\n",
        "@dataclass\n",
        "class SentinelCollator:\n",
        "    def __call__(self, batch: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
        "        out = {}\n",
        "        for k in (\"input_ids\", \"attention_mask\", \"labels\"):\n",
        "            out[k] = torch.stack([b[k] for b in batch])\n",
        "        feats = [\n",
        "            b[\"heuristic_feats\"] if isinstance(b[\"heuristic_feats\"], torch.Tensor)\n",
        "            else torch.tensor(b[\"heuristic_feats\"], dtype=torch.float32)\n",
        "            for b in batch\n",
        "        ]\n",
        "        out[\"heuristic_feats\"] = torch.stack(feats).to(torch.float32)\n",
        "        return out\n",
        "\n",
        "collator = SentinelCollator()\n",
        "\n",
        "# -------------------------\n",
        "# Load data (raw + tokenized with heuristic feats)\n",
        "# -------------------------\n",
        "def _load_data():\n",
        "    assert os.path.isfile(TEST_FILE), \"Test CSV not found.\"\n",
        "    data_files = {}\n",
        "    if os.path.isfile(TRAIN_FILE): data_files['train'] = TRAIN_FILE\n",
        "    if os.path.isfile(VAL_FILE):   data_files['validation'] = VAL_FILE\n",
        "    data_files['test'] = TEST_FILE\n",
        "    raw = load_dataset(\"csv\", data_files=data_files)\n",
        "\n",
        "    # try to read base model name from sentinel checkpoint config\n",
        "    try:\n",
        "        cfg_on_disk = SentinelConfig.from_pretrained(SENTINEL_CHECKPOINT)\n",
        "        base_model = cfg_on_disk.base_model_name\n",
        "        num_labels = cfg_on_disk.num_labels\n",
        "        h_dim = cfg_on_disk.heuristic_dim\n",
        "        global HEURISTIC_DIM\n",
        "        HEURISTIC_DIM = h_dim  # sync to saved config\n",
        "    except Exception:\n",
        "        base_model = \"xlm-roberta-base\"\n",
        "        num_labels = 2\n",
        "\n",
        "    tok = AutoTokenizer.from_pretrained(base_model)\n",
        "    def tok_map(batch):\n",
        "        enc = tok(batch[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
        "        enc[\"heuristic_feats\"] = [build_heuristic_features(t) for t in batch[\"text\"]]\n",
        "        return enc\n",
        "\n",
        "    tokenized = raw.map(tok_map, batched=True)\n",
        "    if \"label\" in tokenized[\"test\"].column_names:\n",
        "        tokenized = tokenized.rename_column(\"label\", \"labels\")\n",
        "    tokenized.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\", \"heuristic_feats\"])\n",
        "    return raw, tokenized, tok, base_model, num_labels\n",
        "\n",
        "raw_datasets, tokenized_datasets, tokenizer, BASE_MODEL_NAME, NUM_LABELS = _load_data()\n",
        "\n",
        "# -------------------------\n",
        "# Load Sentinel model from checkpoint\n",
        "# -------------------------\n",
        "def load_sentinel_for_eval(checkpoint_dir: str):\n",
        "    cfg = SentinelConfig.from_pretrained(checkpoint_dir)\n",
        "    model = SentinelModel.from_pretrained(checkpoint_dir, config=cfg)  # will load weights if saved with Trainer\n",
        "    model.to(device)\n",
        "    return model, cfg\n",
        "\n",
        "model, cfg_loaded = load_sentinel_for_eval(SENTINEL_CHECKPOINT)\n",
        "\n",
        "eval_args = TrainingArguments(\n",
        "    output_dir=\"./tmp_eval_sentinel\",\n",
        "    per_device_eval_batch_size=32,\n",
        "    do_train=False, do_eval=True,\n",
        "    report_to=\"none\",\n",
        "    logging_strategy=\"no\",\n",
        "    disable_tqdm=True\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=eval_args,\n",
        "    data_collator=collator,\n",
        "    tokenizer=tokenizer,  # fine for eval\n",
        "    compute_metrics=None\n",
        ")\n",
        "\n",
        "# -------------------------\n",
        "# Attack tools (structural / semantic / feature-targeted)\n",
        "# -------------------------\n",
        "structural_typo   = nac.KeyboardAug()\n",
        "structural_insert = nac.RandomCharAug(action=\"insert\")\n",
        "\n",
        "def attack_structural_typo(text):\n",
        "    try: return structural_typo.augment(text)[0]\n",
        "    except Exception: return text\n",
        "\n",
        "def attack_structural_insert(text):\n",
        "    try: return structural_insert.augment(text)[0]\n",
        "    except Exception: return text\n",
        "\n",
        "def attack_structural_case(text, p_char=0.15):\n",
        "    if not isinstance(text, str): return text\n",
        "    out = []\n",
        "    for ch in text:\n",
        "        if ch.isalpha() and random.random() < p_char:\n",
        "            out.append(ch.upper() if ch.islower() else ch.lower())\n",
        "        else:\n",
        "            out.append(ch)\n",
        "    return \"\".join(out)\n",
        "\n",
        "synonym_aug = SynonymAug(aug_src=\"wordnet\", aug_p=0.15)\n",
        "def attack_semantic_synonym(text):\n",
        "    try:\n",
        "        out = synonym_aug.augment(text)\n",
        "        return out[0] if isinstance(out, list) and len(out) > 0 else out\n",
        "    except Exception:\n",
        "        if not isinstance(text, str): return text\n",
        "        toks = text.split()\n",
        "        idxs = list(range(len(toks))); random.shuffle(idxs)\n",
        "        for i in idxs:\n",
        "            tok = toks[i]\n",
        "            synsets = wn.synsets(tok)\n",
        "            if not synsets: continue\n",
        "            lemmas = []\n",
        "            for syn in synsets[:3]:\n",
        "                for l in syn.lemmas():\n",
        "                    cand = l.name().replace('_', ' ')\n",
        "                    if cand.lower() != tok.lower():\n",
        "                        lemmas.append(cand)\n",
        "            if lemmas:\n",
        "                toks[i] = random.choice(lemmas); break\n",
        "        return \" \".join(toks)\n",
        "\n",
        "CODED_LEXICON = {\n",
        "    r'\\bmuslims?\\b': 'skittles',\n",
        "    r'\\bblack( people)?\\b': 'googles',\n",
        "    r'\\bjews?\\b': 'skypes',\n",
        "    r'\\bmexicans?\\b': 'bings',\n",
        "}\n",
        "CODED_PATTERNS = [(re.compile(k, flags=re.IGNORECASE), v) for k, v in CODED_LEXICON.items()]\n",
        "def attack_semantic_coded(text):\n",
        "    if not isinstance(text, str): return text\n",
        "    out = text\n",
        "    for pat, repl in CODED_PATTERNS:\n",
        "        out = pat.sub(repl, out)\n",
        "    return out\n",
        "\n",
        "SLUR_LEXICON = [\n",
        "    r\"\\bmongol(s)?\\b\", r\"\\bretard(s|ed)?\\b\", r\"\\btolol\\b\", r\"\\bkontol\\b\",\n",
        "    r\"\\bbajingan\\b\", r\"\\bbabi\\b\", r\"\\bbhen ?chod\\b\", r\"\\bmadar ?chod\\b\",\n",
        "    r\"\\brandi\\b\", r\"\\bperra\\b\", r\"\\bzorra\\b\", r\"\\bputa\\b\"\n",
        "]\n",
        "SLUR_PATTERNS = [re.compile(p, flags=re.IGNORECASE) for p in SLUR_LEXICON]\n",
        "def attack_feature_slur_removal(text, replacement='[removed]'):\n",
        "    if not isinstance(text, str): return text\n",
        "    out = text\n",
        "    for pat in SLUR_PATTERNS:\n",
        "        out = pat.sub(replacement, out)\n",
        "    return out\n",
        "\n",
        "def apply_attack_texts(texts, attack_fn, desc=\"Attacking\"):\n",
        "    return [attack_fn(t) for t in tqdm(texts, desc=desc, leave=False)]\n",
        "\n",
        "# -------------------------\n",
        "# Per-class metrics helper\n",
        "# -------------------------\n",
        "def _per_class_metrics(y_true, y_pred, labels=(0, 1)):\n",
        "    p, r, f1, sup = precision_recall_fscore_support(y_true, y_pred, labels=list(labels), zero_division=0)\n",
        "    return [\n",
        "        {\"label\": int(lbl), \"precision\": float(p[i]), \"recall\": float(r[i]), \"f1\": float(f1[i]), \"support\": int(sup[i])}\n",
        "        for i, lbl in enumerate(labels)\n",
        "    ]\n",
        "\n",
        "# -------------------------\n",
        "# Eval helpers\n",
        "# -------------------------\n",
        "def evaluate_on_texts(trainer, tokenizer, texts, labels, max_length=128):\n",
        "    # Re-tokenize and rebuild heuristic feats for the (possibly) attacked texts\n",
        "    enc = tokenizer(texts, padding='max_length', truncation=True, max_length=max_length)\n",
        "    feats = [build_heuristic_features(t) for t in texts]\n",
        "    ds = Dataset.from_dict({\n",
        "        \"input_ids\": enc[\"input_ids\"],\n",
        "        \"attention_mask\": enc[\"attention_mask\"],\n",
        "        \"heuristic_feats\": feats,\n",
        "        \"label\": labels\n",
        "    })\n",
        "    ds = ds.rename_column(\"label\", \"labels\")\n",
        "    ds.set_format(type=\"torch\", columns=[\"input_ids\",\"attention_mask\",\"labels\",\"heuristic_feats\"])\n",
        "    preds_out = trainer.predict(ds)\n",
        "    logits = preds_out.predictions\n",
        "    preds = np.argmax(logits, axis=-1)\n",
        "    return preds, logits\n",
        "\n",
        "# -------------------------\n",
        "# Attack runner (Sentinel only)\n",
        "# -------------------------\n",
        "def run_attack_and_report(\n",
        "    trainer,\n",
        "    tokenizer,\n",
        "    raw_test_dataset,\n",
        "    tokenized_test_dataset,\n",
        "    attack_fn,\n",
        "    attack_name,\n",
        "    attack_mode='tp',           # 'tp' (True positives only) or 'full'\n",
        "    save_dir=\"attack_reports/sentinel\"\n",
        "):\n",
        "    # Clean predictions (fast) on pre-tokenized test set\n",
        "    clean_out = trainer.predict(tokenized_test_dataset)\n",
        "    clean_logits = clean_out.predictions\n",
        "    clean_preds  = np.argmax(clean_logits, axis=-1)\n",
        "    clean_labels = clean_out.label_ids\n",
        "\n",
        "    # Per-class on clean\n",
        "    clean_per_class = _per_class_metrics(clean_labels, clean_preds)\n",
        "\n",
        "    # Raw texts/labels\n",
        "    raw_texts  = list(raw_test_dataset['text'])\n",
        "    raw_labels = list(raw_test_dataset['label'])\n",
        "    n = len(raw_texts)\n",
        "    assert n == len(clean_preds) == len(clean_labels)\n",
        "\n",
        "    # Target indices\n",
        "    if attack_mode == 'tp':\n",
        "        target_indices = [i for i, (lab, pred) in enumerate(zip(clean_labels, clean_preds)) if lab == 1 and pred == 1]\n",
        "    elif attack_mode == 'full':\n",
        "        target_indices = list(range(n))\n",
        "    else:\n",
        "        raise ValueError(\"attack_mode must be 'tp' or 'full'\")\n",
        "\n",
        "    if len(target_indices) == 0:\n",
        "        os.makedirs(save_dir, exist_ok=True)\n",
        "        report = {\n",
        "            \"attack_name\": attack_name, \"attack_mode\": attack_mode,\n",
        "            \"clean_per_class\": clean_per_class,\n",
        "            \"attacked_per_class\": None,\n",
        "            \"attack_changed\": 0,\n",
        "            \"tp_count_targeted\": 0,\n",
        "            \"samples_flipped\": None,\n",
        "            \"attack_success_rate\": None,\n",
        "            \"note\": \"no targets\"\n",
        "        }\n",
        "        with open(os.path.join(save_dir, f\"{attack_name}_{attack_mode}.json\"), \"w\") as f:\n",
        "            json.dump(report, f, indent=2)\n",
        "        return report\n",
        "\n",
        "    # Apply attack to targeted subset\n",
        "    attacked_texts = raw_texts.copy()\n",
        "    to_attack = [raw_texts[i] for i in target_indices]\n",
        "    attacked_outs = apply_attack_texts(to_attack, attack_fn, desc=attack_name)\n",
        "    attack_count = 0\n",
        "    for idx, new_text in zip(target_indices, attacked_outs):\n",
        "        if new_text != raw_texts[idx]:\n",
        "            attacked_texts[idx] = new_text\n",
        "            attack_count += 1\n",
        "\n",
        "    if attack_count == 0:\n",
        "        os.makedirs(save_dir, exist_ok=True)\n",
        "        report = {\n",
        "            \"attack_name\": attack_name, \"attack_mode\": attack_mode,\n",
        "            \"clean_per_class\": clean_per_class,\n",
        "            \"attacked_per_class\": None,\n",
        "            \"attack_changed\": 0,\n",
        "            \"tp_count_targeted\": len(target_indices),\n",
        "            \"samples_flipped\": None,\n",
        "            \"attack_success_rate\": None,\n",
        "            \"note\": \"no modification made by attack_fn\"\n",
        "        }\n",
        "        with open(os.path.join(save_dir, f\"{attack_name}_{attack_mode}.json\"), \"w\") as f:\n",
        "            json.dump(report, f, indent=2)\n",
        "        return report\n",
        "\n",
        "    # Evaluate attacked set (re-tokenize + rebuild heuristic feats)\n",
        "    attacked_preds, attacked_logits = evaluate_on_texts(trainer, tokenizer, attacked_texts, raw_labels)\n",
        "    attacked_per_class = _per_class_metrics(raw_labels, attacked_preds)\n",
        "\n",
        "    # ASR for TP mode\n",
        "    asr = None; samples_flipped = None\n",
        "    if attack_mode == 'tp':\n",
        "        attacked_for_targets_preds = [attacked_preds[i] for i in target_indices]\n",
        "        samples_flipped = sum(1 for p in attacked_for_targets_preds if p == 0)\n",
        "        asr = samples_flipped / len(target_indices)\n",
        "\n",
        "    report = {\n",
        "        \"attack_name\": attack_name,\n",
        "        \"attack_mode\": attack_mode,\n",
        "        \"clean_per_class\": clean_per_class,\n",
        "        \"attacked_per_class\": attacked_per_class,\n",
        "        \"attack_changed\": attack_count,\n",
        "        \"tp_count_targeted\": len(target_indices),\n",
        "        \"samples_flipped\": int(samples_flipped) if samples_flipped is not None else None,\n",
        "        \"attack_success_rate\": float(asr) if asr is not None else None\n",
        "    }\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    with open(os.path.join(save_dir, f\"{attack_name}_{attack_mode}.json\"), \"w\") as f:\n",
        "        json.dump(report, f, indent=2)\n",
        "    return report\n",
        "\n",
        "def run_all_attacks_sentinel(\n",
        "    trainer,\n",
        "    tokenizer,\n",
        "    raw_test_dataset,\n",
        "    tokenized_test_dataset,\n",
        "    save_dir=\"attack_reports/sentinel\",\n",
        "    attack_mode='tp',\n",
        "    attacks_to_run=None\n",
        "):\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    attacks = attacks_to_run or [\n",
        "        (attack_structural_typo,          \"structural_typo\"),\n",
        "        (attack_structural_insert,        \"structural_insert\"),\n",
        "        (attack_structural_case,          \"structural_case\"),\n",
        "        (attack_semantic_synonym,         \"semantic_synonym\"),\n",
        "        (attack_semantic_coded,           \"semantic_coded\"),\n",
        "        (lambda t: attack_feature_slur_removal(t, \"[removed]\"), \"feature_slur_removal\"),\n",
        "    ]\n",
        "    suite_report = {\n",
        "        \"model\": \"SentinelModel\",\n",
        "        \"checkpoint\": SENTINEL_CHECKPOINT,\n",
        "        \"attack_mode\": attack_mode,\n",
        "        \"attacks\": []\n",
        "    }\n",
        "    for fn, name in attacks:\n",
        "        print(f\"Running attack: {name} (mode={attack_mode})\")\n",
        "        rpt = run_attack_and_report(trainer, tokenizer, raw_test_dataset, tokenized_test_dataset, fn, name, attack_mode, save_dir)\n",
        "        if rpt is not None:\n",
        "            suite_report[\"attacks\"].append(rpt)\n",
        "    with open(os.path.join(save_dir, f\"suite_report_sentinel_{attack_mode}.json\"), \"w\") as f:\n",
        "        json.dump(suite_report, f, indent=2)\n",
        "    return suite_report\n",
        "\n",
        "# -------------------------\n",
        "# RUN (Sentinel only)\n",
        "# -------------------------\n",
        "print(\"Sentinel eval trainer ready. Running attacks...\")\n",
        "sentinel_report = run_all_attacks_sentinel(\n",
        "    trainer=trainer,\n",
        "    tokenizer=tokenizer,\n",
        "    raw_test_dataset=raw_datasets[\"test\"],\n",
        "    tokenized_test_dataset=tokenized_datasets[\"test\"],\n",
        "    save_dir=\"attack_reports/sentinel\",\n",
        "    attack_mode=\"tp\"   # change to \"full\" to attack all test texts\n",
        ")\n",
        "print(\" Sentinel attacks finished. Reports -> attack_reports/sentinel\")\n"
      ],
      "metadata": {
        "id": "zLNhIHppoTDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive', force_remount=False)\n",
        "\n",
        "import os, shutil\n",
        "\n",
        "src = \"/content/attack_reports\"                     # where your reports live in Colab\n",
        "dst = \"/content/drive/MyDrive/hate/attack_reports\"  # destination in Drive\n",
        "\n",
        "if not os.path.exists(src):\n",
        "    raise FileNotFoundError(f\"Source folder not found: {src}\")\n",
        "\n",
        "os.makedirs(os.path.dirname(dst), exist_ok=True)\n",
        "shutil.copytree(src, dst, dirs_exist_ok=True)  # dirs_exist_ok needs Python 3.8+\n",
        "\n",
        "print(f\" Copied '{src}'  '{dst}'\")\n"
      ],
      "metadata": {
        "id": "j6ab1dnQo0KL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === 3-Model ROC & PR Curves (600 dpi, saves PNG/PDF) ===\n",
        "# Paths\n",
        "DRIVE_BASE           = \"/content/drive/MyDrive/hate\"\n",
        "TRAIN_FILE           = f\"{DRIVE_BASE}/train.csv\"\n",
        "VAL_FILE             = f\"{DRIVE_BASE}/val.csv\"\n",
        "TEST_FILE            = f\"{DRIVE_BASE}/test.csv\"\n",
        "\n",
        "BASELINE_CKPT        = f\"{DRIVE_BASE}/models/step1_bert_baseline\"\n",
        "GATED_CKPT           = f\"{DRIVE_BASE}/models/step1_gated_fusion\"\n",
        "SENTINEL_CKPT        = f\"{DRIVE_BASE}/models/sentinel_xlmr\"\n",
        "\n",
        "FIG_DIR              = f\"{DRIVE_BASE}/fig\"\n",
        "\n",
        "# Deps\n",
        "!pip -q install transformers datasets scikit-learn matplotlib\n",
        "\n",
        "import os, re, math, json, numpy as np, torch, torch.nn as nn\n",
        "from typing import Dict, Any, List, Optional\n",
        "from dataclasses import dataclass\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from datasets import load_dataset, Dataset\n",
        "from sklearn.metrics import roc_curve, roc_auc_score, precision_recall_curve, average_precision_score\n",
        "\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoModel, AutoConfig,\n",
        "    AutoModelForSequenceClassification,\n",
        "    PretrainedConfig, PreTrainedModel,\n",
        "    TrainingArguments, Trainer\n",
        ")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "os.makedirs(FIG_DIR, exist_ok=True)\n",
        "\n",
        "# -------------------------\n",
        "# Load test data\n",
        "# -------------------------\n",
        "assert os.path.isfile(TEST_FILE), \"TEST_FILE not found.\"\n",
        "raw = load_dataset(\"csv\", data_files={\"test\": TEST_FILE})\n",
        "assert \"text\" in raw[\"test\"].column_names and \"label\" in raw[\"test\"].column_names, \"CSV must have 'text' and 'label' columns.\"\n",
        "\n",
        "texts  = list(raw[\"test\"][\"text\"])\n",
        "y_true = np.array(list(raw[\"test\"][\"label\"]), dtype=int)\n",
        "\n",
        "# -------------------------\n",
        "# Sentinel model/types (must match training)\n",
        "# -------------------------\n",
        "class SentinelConfig(PretrainedConfig):\n",
        "    model_type = \"sentinel_fusion\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        base_model_name: str = \"xlm-roberta-base\",\n",
        "        num_labels: int = 2,\n",
        "        heuristic_dim: int = 32,\n",
        "        heuristic_hidden: int = 256,\n",
        "        causal_hidden: int = 256,\n",
        "        attn_heads: int = 8,\n",
        "        aux_causal_loss_weight: float = 0.2,\n",
        "        **kwargs\n",
        "    ):\n",
        "        super().__init__(**kwargs)\n",
        "        self.base_model_name = base_model_name\n",
        "        self.num_labels = num_labels\n",
        "        self.heuristic_dim = heuristic_dim\n",
        "        self.heuristic_hidden = heuristic_hidden\n",
        "        self.causal_hidden = causal_hidden\n",
        "        self.attn_heads = attn_heads\n",
        "        self.aux_causal_loss_weight = aux_causal_loss_weight\n",
        "\n",
        "class SentinelModel(PreTrainedModel):\n",
        "    config_class = SentinelConfig\n",
        "    def __init__(self, config: SentinelConfig):\n",
        "        super().__init__(config)\n",
        "        self.base_cfg = AutoConfig.from_pretrained(config.base_model_name)\n",
        "        self.encoder = AutoModel.from_pretrained(config.base_model_name, config=self.base_cfg)\n",
        "        hidden = self.base_cfg.hidden_size\n",
        "\n",
        "        self.heuristic_proj = nn.Sequential(\n",
        "            nn.Linear(config.heuristic_dim, config.heuristic_hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(config.heuristic_hidden, hidden),\n",
        "            nn.LayerNorm(hidden)\n",
        "        )\n",
        "        self.causal_mlp = nn.Sequential(\n",
        "            nn.Linear(hidden, config.causal_hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(config.causal_hidden, hidden),\n",
        "            nn.LayerNorm(hidden)\n",
        "        )\n",
        "        self.causal_aux_head = nn.Linear(hidden, 2)\n",
        "        self.xattn = nn.MultiheadAttention(embed_dim=hidden, num_heads=config.attn_heads, batch_first=True)\n",
        "        self.dropout = nn.Dropout(getattr(self.base_cfg, \"hidden_dropout_prob\", 0.1))\n",
        "        self.classifier = nn.Linear(hidden, config.num_labels)\n",
        "        self.post_init()\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        heuristic_feats=None,\n",
        "        labels=None,\n",
        "        **kwargs\n",
        "    ):\n",
        "        enc = self.encoder(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
        "        last_hidden = enc.last_hidden_state\n",
        "        h_cls = last_hidden[:, 0, :]\n",
        "        h_heu = self.heuristic_proj(heuristic_feats)\n",
        "        h_cau = self.causal_mlp(h_cls)\n",
        "        Q  = h_cls.unsqueeze(1)\n",
        "        KV = torch.stack([h_heu, h_cau], dim=1)\n",
        "        fused, _ = self.xattn(Q, KV, KV)\n",
        "        fused = fused.squeeze(1)\n",
        "        logits = self.classifier(self.dropout(fused))\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss = nn.CrossEntropyLoss()(logits, labels)\n",
        "        return {\"loss\": loss, \"logits\": logits}\n",
        "\n",
        "# -------------------------\n",
        "# Gated fusion model/types (from earlier)\n",
        "# -------------------------\n",
        "class GFConfig(PretrainedConfig):\n",
        "    model_type = \"gated_fusion_wrapper\"\n",
        "    def __init__(self, base_model_name: str = \"bert-base-multilingual-cased\", num_labels: int = 2, gate_hidden: int = 256, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.base_model_name = base_model_name\n",
        "        self.num_labels = num_labels\n",
        "        self.gate_hidden = gate_hidden\n",
        "\n",
        "class GatedFusionForSequenceClassification(PreTrainedModel):\n",
        "    config_class = GFConfig\n",
        "    def __init__(self, config: GFConfig):\n",
        "        super().__init__(config)\n",
        "        self.base_cfg = AutoConfig.from_pretrained(config.base_model_name)\n",
        "        self.encoder = AutoModel.from_pretrained(config.base_model_name, config=self.base_cfg)\n",
        "        hidden = self.base_cfg.hidden_size\n",
        "        self.gate_mlp = nn.Sequential(\n",
        "            nn.Linear(2 * hidden, config.gate_hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(config.gate_hidden, hidden),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        self.dropout = nn.Dropout(getattr(self.base_cfg, \"hidden_dropout_prob\", 0.1))\n",
        "        self.classifier = nn.Linear(hidden, config.num_labels)\n",
        "        self.post_init()\n",
        "\n",
        "    @staticmethod\n",
        "    def masked_mean(last_hidden_state, attention_mask):\n",
        "        mask = attention_mask.unsqueeze(-1).type_as(last_hidden_state)\n",
        "        summed = (last_hidden_state * mask).sum(dim=1)\n",
        "        denom = mask.sum(dim=1).clamp(min=1e-6)\n",
        "        return summed / denom\n",
        "\n",
        "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, labels=None, **kwargs):\n",
        "        allowed = {\"position_ids\",\"head_mask\",\"inputs_embeds\",\"output_attentions\",\"output_hidden_states\",\"return_dict\",\"past_key_values\",\"encoder_hidden_states\",\"encoder_attention_mask\"}\n",
        "        safe_kwargs = {k: v for k, v in kwargs.items() if k in allowed}\n",
        "        safe_kwargs.pop(\"num_items_in_batch\", None)\n",
        "\n",
        "        enc = self.encoder(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, **safe_kwargs)\n",
        "        h_cls = enc.last_hidden_state[:, 0, :]\n",
        "        h_mean = self.masked_mean(enc.last_hidden_state, attention_mask)\n",
        "        gate_inp = torch.cat([h_cls, h_mean], dim=-1)\n",
        "        g = self.gate_mlp(gate_inp)\n",
        "        fused = g * h_cls + (1.0 - g) * h_mean\n",
        "        logits = self.classifier(self.dropout(fused))\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss = nn.CrossEntropyLoss()(logits, labels)\n",
        "        return {\"loss\": loss, \"logits\": logits}\n",
        "\n",
        "# -------------------------\n",
        "# Heuristic features (Sentinel)  keep in sync with training\n",
        "# -------------------------\n",
        "_SLUR_REGEXES = [\n",
        "    r\"\\bmongol(s)?\\b\", r\"\\bretard(s|ed)?\\b\", r\"\\btolol\\b\", r\"\\bkontol\\b\",\n",
        "    r\"\\bbajingan\\b\", r\"\\bbabi\\b\", r\"\\bbhen ?chod\\b\", r\"\\bmadar ?chod\\b\",\n",
        "    r\"\\brandi\\b\", r\"\\bperra\\b\", r\"\\bzorra\\b\", r\"\\bputa\\b\"\n",
        "]\n",
        "_SLUR_PATTERNS = [re.compile(p, re.IGNORECASE) for p in _SLUR_REGEXES]\n",
        "\n",
        "def _build_heuristic_features(text: str, H: int) -> np.ndarray:\n",
        "    if not isinstance(text, str):\n",
        "        text = \"\" if text is None else str(text)\n",
        "    length = len(text)\n",
        "    words  = text.split()\n",
        "    n_words = max(1, len(words))\n",
        "    upper  = sum(1 for c in text if c.isalpha() and c.isupper())\n",
        "    digits = sum(1 for c in text if c.isdigit())\n",
        "    punct  = sum(1 for c in text if c in \".,;:!?\")\n",
        "    alpha_count = max(1, sum(c.isalpha() for c in text))\n",
        "    upper_ratio = upper / alpha_count\n",
        "    digit_ratio = digits / max(1, len(text))\n",
        "    punct_ratio = punct / max(1, len(text))\n",
        "    slur_hits = 0\n",
        "    for pat in _SLUR_PATTERNS:\n",
        "        slur_hits += len(pat.findall(text))\n",
        "    slur_density = slur_hits / n_words\n",
        "    cues = sum(text.lower().count(k) for k in [\"kill\", \"die\", \"trash\", \"dirty\", \"dog\", \"pig\", \"scum\", \"hate\"])\n",
        "    cue_density = cues / n_words\n",
        "    base_feats = np.array([\n",
        "        math.log1p(length), math.log1p(n_words), upper, digits, punct,\n",
        "        upper_ratio, digit_ratio, punct_ratio,\n",
        "        slur_hits, slur_density, cues, cue_density\n",
        "    ], dtype=np.float32)\n",
        "    if base_feats.shape[0] < H:\n",
        "        pad = np.zeros(H - base_feats.shape[0], dtype=np.float32)\n",
        "        feats = np.concatenate([base_feats, pad])\n",
        "    else:\n",
        "        feats = base_feats[:H]\n",
        "    return feats\n",
        "\n",
        "@dataclass\n",
        "class SentinelCollator:\n",
        "    def __call__(self, batch: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
        "        out = {\n",
        "            \"input_ids\": torch.stack([b[\"input_ids\"] for b in batch]),\n",
        "            \"attention_mask\": torch.stack([b[\"attention_mask\"] for b in batch]),\n",
        "            \"labels\": torch.stack([b[\"labels\"] for b in batch])\n",
        "        }\n",
        "        feats = [\n",
        "            (b[\"heuristic_feats\"] if isinstance(b[\"heuristic_feats\"], torch.Tensor)\n",
        "             else torch.tensor(b[\"heuristic_feats\"], dtype=torch.float32))\n",
        "            for b in batch\n",
        "        ]\n",
        "        out[\"heuristic_feats\"] = torch.stack(feats).to(torch.float32)\n",
        "        return out\n",
        "\n",
        "# -------------------------\n",
        "# Utility: get probabilities for positive class (label=1)\n",
        "# -------------------------\n",
        "def _softmax_np(logits: np.ndarray) -> np.ndarray:\n",
        "    z = logits - logits.max(axis=1, keepdims=True)\n",
        "    e = np.exp(z)\n",
        "    p = e / e.sum(axis=1, keepdims=True)\n",
        "    return p\n",
        "\n",
        "def predict_probs_baseline_or_gated(ckpt: str, texts: list) -> np.ndarray:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(ckpt)\n",
        "    # Try plain classifier first; fall back to custom gated head if needed\n",
        "    try:\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(ckpt)\n",
        "    except Exception:\n",
        "        cfg = GFConfig.from_pretrained(ckpt)\n",
        "        model = GatedFusionForSequenceClassification.from_pretrained(ckpt, config=cfg)\n",
        "    model.to(device).eval()\n",
        "\n",
        "    # Return lists (not tensors) for Dataset.from_dict\n",
        "    enc = tokenizer(texts, padding=\"max_length\", truncation=True, max_length=128)\n",
        "    ds = Dataset.from_dict({\n",
        "        \"input_ids\": enc[\"input_ids\"],\n",
        "        \"attention_mask\": enc[\"attention_mask\"],\n",
        "        \"labels\": y_true.tolist(),   # already named 'labels' -> no rename\n",
        "    })\n",
        "    ds.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "\n",
        "    args = TrainingArguments(\n",
        "        output_dir=\"./tmp_eval_generic\",\n",
        "        per_device_eval_batch_size=64,\n",
        "        report_to=\"none\", logging_strategy=\"no\", disable_tqdm=True\n",
        "    )\n",
        "    tr = Trainer(model=model, args=args, compute_metrics=None, tokenizer=tokenizer)\n",
        "    out = tr.predict(ds)\n",
        "    probs = _softmax_np(out.predictions)[:, 1]\n",
        "    return probs\n",
        "\n",
        "\n",
        "def predict_probs_sentinel(ckpt: str, texts: list) -> np.ndarray:\n",
        "    cfg = SentinelConfig.from_pretrained(ckpt)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(cfg.base_model_name)\n",
        "    model = SentinelModel.from_pretrained(ckpt, config=cfg).to(device).eval()\n",
        "\n",
        "    enc = tokenizer(texts, padding=\"max_length\", truncation=True, max_length=128)\n",
        "    feats = [_build_heuristic_features(t, cfg.heuristic_dim) for t in texts]\n",
        "    ds = Dataset.from_dict({\n",
        "        \"input_ids\": enc[\"input_ids\"],\n",
        "        \"attention_mask\": enc[\"attention_mask\"],\n",
        "        \"heuristic_feats\": feats,\n",
        "        \"labels\": y_true.tolist(),   # already 'labels'\n",
        "    })\n",
        "    ds.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\", \"heuristic_feats\"])\n",
        "\n",
        "    args = TrainingArguments(\n",
        "        output_dir=\"./tmp_eval_sentinel\",\n",
        "        per_device_eval_batch_size=64,\n",
        "        report_to=\"none\", logging_strategy=\"no\", disable_tqdm=True\n",
        "    )\n",
        "    tr = Trainer(\n",
        "        model=model, args=args, compute_metrics=None,\n",
        "        data_collator=SentinelCollator(), tokenizer=tokenizer\n",
        "    )\n",
        "    out = tr.predict(ds)\n",
        "    probs = _softmax_np(out.predictions)[:, 1]\n",
        "    return probs\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Get probabilities for each model\n",
        "# -------------------------\n",
        "print(\"Scoring test set with Baseline...\")\n",
        "probs_baseline = predict_probs_baseline_or_gated(BASELINE_CKPT, texts)\n",
        "\n",
        "print(\"Scoring test set with Gated Fusion...\")\n",
        "probs_gated = predict_probs_baseline_or_gated(GATED_CKPT, texts)\n",
        "\n",
        "print(\"Scoring test set with Sentinel...\")\n",
        "probs_sentinel = predict_probs_sentinel(SENTINEL_CKPT, texts)\n",
        "\n",
        "# -------------------------\n",
        "# Compute curves & metrics\n",
        "# -------------------------\n",
        "def compute_roc_pr(y_true, y_score):\n",
        "    fpr, tpr, _ = roc_curve(y_true, y_score, pos_label=1)\n",
        "    roc_auc = roc_auc_score(y_true, y_score)\n",
        "    prec, rec, _ = precision_recall_curve(y_true, y_score, pos_label=1)\n",
        "    ap = average_precision_score(y_true, y_score)  # area under PR (AP)\n",
        "    return (fpr, tpr, roc_auc), (rec, prec, ap)\n",
        "\n",
        "roc_pr = {}\n",
        "roc_pr[\"Baseline\"]  = compute_roc_pr(y_true, probs_baseline)\n",
        "roc_pr[\"Gated\"]     = compute_roc_pr(y_true, probs_gated)\n",
        "roc_pr[\"Sentinel\"]  = compute_roc_pr(y_true, probs_sentinel)\n",
        "\n",
        "# -------------------------\n",
        "# Plot ROC (all 3)  600 dpi\n",
        "# -------------------------\n",
        "plt.figure(figsize=(6, 6), dpi=600)\n",
        "for name, ((fpr, tpr, aucv), _) in roc_pr.items():\n",
        "    plt.plot(fpr, tpr, label=f\"{name} (AUC={aucv:.3f})\", linewidth=1.5)\n",
        "plt.plot([0, 1], [0, 1], linestyle=\"--\", linewidth=1.0)\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"ROC Curves (Test)\")\n",
        "plt.legend(loc=\"lower right\", frameon=False)\n",
        "plt.tight_layout()\n",
        "roc_png = os.path.join(FIG_DIR, \"roc_three_models.png\")\n",
        "roc_pdf = os.path.join(FIG_DIR, \"roc_three_models.pdf\")\n",
        "plt.savefig(roc_png, dpi=600)\n",
        "plt.savefig(roc_pdf, dpi=600)\n",
        "plt.close()\n",
        "\n",
        "# -------------------------\n",
        "# Plot PrecisionRecall (all 3)  600 dpi\n",
        "# -------------------------\n",
        "plt.figure(figsize=(6, 6), dpi=600)\n",
        "for name, (_, (rec, prec, ap)) in roc_pr.items():\n",
        "    plt.plot(rec, prec, label=f\"{name} (AP={ap:.3f})\", linewidth=1.5)\n",
        "# Baseline of PR depends on class prevalence; no trivial diagonal\n",
        "plt.xlabel(\"Recall\")\n",
        "plt.ylabel(\"Precision\")\n",
        "plt.title(\"PrecisionRecall Curves (Test)\")\n",
        "plt.legend(loc=\"lower left\", frameon=False)\n",
        "plt.tight_layout()\n",
        "pr_png = os.path.join(FIG_DIR, \"pr_three_models.png\")\n",
        "pr_pdf = os.path.join(FIG_DIR, \"pr_three_models.pdf\")\n",
        "plt.savefig(pr_png, dpi=600)\n",
        "plt.savefig(pr_pdf, dpi=600)\n",
        "plt.close()\n",
        "\n",
        "print(\"Saved figures:\")\n",
        "print(\" -\", roc_png)\n",
        "print(\" -\", roc_pdf)\n",
        "print(\" -\", pr_png)\n",
        "print(\" -\", pr_pdf)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "bqZO12o1wjot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# BAR CHARTS: Clean vs Adv + Attack-wise (ASR)\n",
        "# Saves 600dpi figs to /content/drive/MyDrive/hate/figs\n",
        "# ================================\n",
        "\n",
        "import os, json, math, glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 0) Mount Drive (skip if already mounted)\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')\n",
        "\n",
        "BASE = \"/content/drive/MyDrive/hate\"\n",
        "REPORT_ROOT = os.path.join(BASE, \"attack_reports\")\n",
        "FIG_DIR = os.path.join(BASE, \"figs\")\n",
        "os.makedirs(FIG_DIR, exist_ok=True)\n",
        "\n",
        "# 1) Discover model report folders (e.g., baseline, gated, sentinel)\n",
        "if not os.path.isdir(REPORT_ROOT):\n",
        "    raise FileNotFoundError(f\"Report root not found: {REPORT_ROOT}\")\n",
        "\n",
        "model_dirs = [d for d in glob.glob(os.path.join(REPORT_ROOT, \"*\")) if os.path.isdir(d)]\n",
        "if not model_dirs:\n",
        "    raise RuntimeError(f\"No model subfolders under {REPORT_ROOT}\")\n",
        "\n",
        "# Friendly labels (optional mapping)\n",
        "label_map = {\n",
        "    \"baseline\": \"Baseline\",\n",
        "    \"gated\": \"Gated Fusion\",\n",
        "    \"sentinel\": \"Sentinel\",\n",
        "}\n",
        "def pretty_model_name(path):\n",
        "    name = os.path.basename(path)\n",
        "    return label_map.get(name.lower(), name)\n",
        "\n",
        "# 2) Parse attack JSONs into a tidy DataFrame\n",
        "rows = []\n",
        "for mdir in model_dirs:\n",
        "    model_name = pretty_model_name(mdir)\n",
        "    for f in glob.glob(os.path.join(mdir, \"*.json\")):\n",
        "        # Skip suite report jsons\n",
        "        base = os.path.basename(f)\n",
        "        if base.startswith(\"suite_report\"):\n",
        "            continue\n",
        "        try:\n",
        "            with open(f, \"r\") as fp:\n",
        "                data = json.load(fp)\n",
        "        except Exception:\n",
        "            continue\n",
        "\n",
        "        attack_name = data.get(\"attack_name\", os.path.splitext(base)[0])\n",
        "\n",
        "        # Compute macro-F1 from per-class metrics arrays\n",
        "        clean_pc = data.get(\"clean_per_class\", [])\n",
        "        attacked_pc = data.get(\"attacked_per_class\", [])\n",
        "\n",
        "        def macro_f1(pc):\n",
        "            if isinstance(pc, list) and pc and isinstance(pc[0], dict) and \"f1\" in pc[0]:\n",
        "                return float(np.mean([float(x.get(\"f1\", 0.0)) for x in pc]))\n",
        "            return np.nan\n",
        "\n",
        "        clean_f1 = macro_f1(clean_pc)\n",
        "        attacked_f1 = macro_f1(attacked_pc)\n",
        "        delta_f1 = np.nan\n",
        "        if not math.isnan(clean_f1) and not math.isnan(attacked_f1):\n",
        "            delta_f1 = attacked_f1 - clean_f1\n",
        "\n",
        "        asr = data.get(\"attack_success_rate\", None)\n",
        "        if asr is None:\n",
        "            asr = np.nan\n",
        "\n",
        "        rows.append({\n",
        "            \"model\": model_name,\n",
        "            \"attack\": attack_name,\n",
        "            \"clean_macro_f1\": clean_f1,\n",
        "            \"attacked_macro_f1\": attacked_f1,\n",
        "            \"delta_macro_f1\": delta_f1,\n",
        "            \"attack_success_rate\": asr\n",
        "        })\n",
        "\n",
        "df = pd.DataFrame(rows)\n",
        "if df.empty:\n",
        "    raise RuntimeError(\"No usable attack JSON files found under attack_reports/*/*.json.\")\n",
        "\n",
        "# Order models nicely if present\n",
        "model_order = [lbl for key, lbl in label_map.items() if lbl in df[\"model\"].unique()]\n",
        "if not model_order:\n",
        "    model_order = sorted(df[\"model\"].unique())\n",
        "\n",
        "# -------------------------------\n",
        "# Figure 1: Mean clean vs attacked F1 per model\n",
        "# -------------------------------\n",
        "agg = df.groupby(\"model\", as_index=False)[[\"clean_macro_f1\", \"attacked_macro_f1\"]].mean(numeric_only=True)\n",
        "agg[\"model\"] = pd.Categorical(agg[\"model\"], categories=model_order, ordered=True)\n",
        "agg = agg.sort_values(\"model\")\n",
        "\n",
        "plt.figure(figsize=(7, 5))\n",
        "x = np.arange(len(agg))\n",
        "w = 0.35\n",
        "plt.bar(x - w/2, agg[\"clean_macro_f1\"], width=w, label=\"Clean macro-F1\")\n",
        "plt.bar(x + w/2, agg[\"attacked_macro_f1\"], width=w, label=\"Attacked macro-F1\")\n",
        "plt.xticks(x, agg[\"model\"], rotation=0)\n",
        "plt.ylabel(\"Macro-F1\")\n",
        "plt.title(\"Clean vs. Adversarial (Mean Macro-F1) by Model\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "fig1_path = os.path.join(FIG_DIR, \"clean_vs_adv_f1_by_model.png\")\n",
        "plt.savefig(fig1_path, dpi=600)\n",
        "plt.close()\n",
        "\n",
        "# -------------------------------\n",
        "# Figure 2: Attack-wise comparison across models\n",
        "# Prefer ASR; fallback to normalized -F1 if ASR missing\n",
        "# -------------------------------\n",
        "df_asr = df.copy()\n",
        "\n",
        "# Fallback: if ASR missing, use normalized -delta_f1 (clip to [0,1]) as a rough proxy\n",
        "mask_missing_asr = df_asr[\"attack_success_rate\"].isna()\n",
        "if mask_missing_asr.any():\n",
        "    approx_asr = (-df_asr.loc[mask_missing_asr, \"delta_macro_f1\"]).clip(lower=0.0, upper=1.0)\n",
        "    df_asr.loc[mask_missing_asr, \"attack_success_rate\"] = approx_asr\n",
        "\n",
        "# Keep common attacks across models for fair plotting (or all if none common)\n",
        "common_attacks = sorted(\n",
        "    set.intersection(*[set(df_asr[df_asr[\"model\"] == m][\"attack\"].unique()) for m in df_asr[\"model\"].unique()])\n",
        ") or sorted(df_asr[\"attack\"].unique())\n",
        "\n",
        "plot_df = df_asr[df_asr[\"attack\"].isin(common_attacks)].copy()\n",
        "plot_df[\"model\"] = pd.Categorical(plot_df[\"model\"], categories=model_order, ordered=True)\n",
        "plot_df = plot_df.sort_values([\"attack\", \"model\"])\n",
        "\n",
        "attacks = common_attacks\n",
        "M = len(model_order)\n",
        "N = len(attacks)\n",
        "bar_w = 0.8 / max(1, M)\n",
        "indices = np.arange(N)\n",
        "\n",
        "plt.figure(figsize=(max(8, N * 0.6), 5))\n",
        "for i, m in enumerate(model_order):\n",
        "    vals = []\n",
        "    for a in attacks:\n",
        "        row = plot_df[(plot_df[\"model\"] == m) & (plot_df[\"attack\"] == a)]\n",
        "        vals.append(float(row[\"attack_success_rate\"].iloc[0]) if not row.empty else np.nan)\n",
        "    vals = np.array(vals, dtype=float)\n",
        "    plt.bar(indices + i * bar_w - (M-1)*bar_w/2, vals, width=bar_w, label=m)\n",
        "\n",
        "plt.xticks(indices, attacks, rotation=35, ha=\"right\")\n",
        "plt.ylabel(\"Attack Success Rate (ASR)\")\n",
        "plt.title(\"Attack-wise Comparison by Model\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "fig2_path = os.path.join(FIG_DIR, \"asr_by_attack_and_model.png\")\n",
        "plt.savefig(fig2_path, dpi=600)\n",
        "plt.close()\n",
        "\n",
        "print(\"Saved 600dpi figures to:\")\n",
        "print(\" -\", fig1_path)\n",
        "print(\" -\", fig2_path)\n"
      ],
      "metadata": {
        "id": "otBOWOM5qKHs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# TABULAR VIEW: per-attack + grouped (3 categories)\n",
        "# ================================\n",
        "import os, json, glob, math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Mount Drive (skip if already mounted)\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')\n",
        "\n",
        "BASE = \"/content/drive/MyDrive/hate\"\n",
        "REPORT_ROOT = os.path.join(BASE, \"attack_reports\")\n",
        "\n",
        "# --- discover model subfolders\n",
        "if not os.path.isdir(REPORT_ROOT):\n",
        "    raise FileNotFoundError(f\"Report root not found: {REPORT_ROOT}\")\n",
        "\n",
        "model_dirs = [d for d in glob.glob(os.path.join(REPORT_ROOT, \"*\")) if os.path.isdir(d)]\n",
        "if not model_dirs:\n",
        "    raise RuntimeError(f\"No model subfolders under {REPORT_ROOT}\")\n",
        "\n",
        "# Pretty names (optional)\n",
        "label_map = {\n",
        "    \"baseline\": \"Baseline\",\n",
        "    \"gated\": \"Gated Fusion\",\n",
        "    \"sentinel\": \"Sentinel\",\n",
        "}\n",
        "def pretty_model_name(path):\n",
        "    name = os.path.basename(path)\n",
        "    return label_map.get(name.lower(), name)\n",
        "\n",
        "# --- attack -> category mapping\n",
        "def attack_category(name: str) -> str:\n",
        "    n = name.lower()\n",
        "    if n in {\"structural_typo\", \"structural_insert\", \"structural_case\"}:\n",
        "        return \"Structural\"\n",
        "    if n in {\"semantic_synonym\", \"semantic_coded\"}:\n",
        "        return \"Semantic/Cue\"\n",
        "    if n in {\"feature_slur_removal\"}:\n",
        "        return \"Feature-Targeted\"\n",
        "    # fallback: guess by keywords\n",
        "    if \"structural\" in n or \"typo\" in n or \"insert\" in n or \"case\" in n:\n",
        "        return \"Structural\"\n",
        "    if \"synonym\" in n or \"coded\" in n or \"semantic\" in n:\n",
        "        return \"Semantic/Cue\"\n",
        "    if \"slur\" in n or \"feature\" in n:\n",
        "        return \"Feature-Targeted\"\n",
        "    return \"Uncategorized\"\n",
        "\n",
        "# --- helpers\n",
        "def macro_f1_from_perclass(per_class_list):\n",
        "    if isinstance(per_class_list, list) and per_class_list and isinstance(per_class_list[0], dict):\n",
        "        vals = [float(x.get(\"f1\", 0.0)) for x in per_class_list]\n",
        "        return float(np.mean(vals)) if len(vals) else np.nan\n",
        "    return np.nan\n",
        "\n",
        "# --- parse all JSONs\n",
        "rows = []\n",
        "for mdir in model_dirs:\n",
        "    model_name = pretty_model_name(mdir)\n",
        "    for f in glob.glob(os.path.join(mdir, \"*.json\")):\n",
        "        base = os.path.basename(f)\n",
        "        if base.startswith(\"suite_report\"):\n",
        "            continue\n",
        "        try:\n",
        "            with open(f, \"r\") as fp:\n",
        "                data = json.load(fp)\n",
        "        except Exception:\n",
        "            continue\n",
        "\n",
        "        attack_name = data.get(\"attack_name\", os.path.splitext(base)[0])\n",
        "\n",
        "        clean_f1 = macro_f1_from_perclass(data.get(\"clean_per_class\", []))\n",
        "        attacked_f1 = macro_f1_from_perclass(data.get(\"attacked_per_class\", []))\n",
        "        dF1 = np.nan\n",
        "        if not math.isnan(clean_f1) and not math.isnan(attacked_f1):\n",
        "            dF1 = attacked_f1 - clean_f1\n",
        "\n",
        "        asr = data.get(\"attack_success_rate\", np.nan)\n",
        "\n",
        "        rows.append({\n",
        "            \"model\": model_name,\n",
        "            \"attack\": attack_name,\n",
        "            \"category\": attack_category(attack_name),\n",
        "            \"clean_macro_f1\": clean_f1,\n",
        "            \"attacked_macro_f1\": attacked_f1,\n",
        "            \"delta_macro_f1\": dF1,\n",
        "            \"ASR\": asr\n",
        "        })\n",
        "\n",
        "df = pd.DataFrame(rows)\n",
        "if df.empty:\n",
        "    raise RuntimeError(\"No usable attack JSON files found.\")\n",
        "\n",
        "# --- tidy ordering\n",
        "model_order = [lbl for lbl in [\"Baseline\", \"Gated Fusion\", \"Sentinel\"] if lbl in df[\"model\"].unique()]\n",
        "if not model_order:\n",
        "    model_order = sorted(df[\"model\"].unique())\n",
        "cat_order = [\"Structural\", \"Semantic/Cue\", \"Feature-Targeted\", \"Uncategorized\"]\n",
        "\n",
        "df[\"model\"] = pd.Categorical(df[\"model\"], categories=model_order, ordered=True)\n",
        "df[\"category\"] = pd.Categorical(df[\"category\"], categories=cat_order, ordered=True)\n",
        "df = df.sort_values([\"model\", \"category\", \"attack\"]).reset_index(drop=True)\n",
        "\n",
        "# --- show per-attack table (rounded)\n",
        "per_attack_cols = [\"model\", \"category\", \"attack\", \"clean_macro_f1\", \"attacked_macro_f1\", \"delta_macro_f1\", \"ASR\"]\n",
        "per_attack_df = df[per_attack_cols].copy()\n",
        "per_attack_df[[\"clean_macro_f1\",\"attacked_macro_f1\",\"delta_macro_f1\",\"ASR\"]] = \\\n",
        "    per_attack_df[[\"clean_macro_f1\",\"attacked_macro_f1\",\"delta_macro_f1\",\"ASR\"]].round(4)\n",
        "\n",
        "print(\"=== Per-attack metrics (Clean vs Attacked, F1, ASR) ===\")\n",
        "display(per_attack_df)\n",
        "\n",
        "# --- grouped summary: mean across attacks per (model, category)\n",
        "grouped = (\n",
        "    df.groupby([\"model\",\"category\"], as_index=False)\n",
        "      .agg(\n",
        "          mean_clean_macro_f1 = (\"clean_macro_f1\", \"mean\"),\n",
        "          mean_attacked_macro_f1 = (\"attacked_macro_f1\", \"mean\"),\n",
        "          mean_delta_macro_f1 = (\"delta_macro_f1\", \"mean\"),\n",
        "          mean_ASR = (\"ASR\", \"mean\"),\n",
        "          n_attacks = (\"attack\", \"nunique\")\n",
        "      )\n",
        ")\n",
        "for c in [\"mean_clean_macro_f1\",\"mean_attacked_macro_f1\",\"mean_delta_macro_f1\",\"mean_ASR\"]:\n",
        "    grouped[c] = grouped[c].round(4)\n",
        "\n",
        "grouped = grouped.sort_values([\"model\",\"category\"])\n",
        "print(\"\\n=== Grouped summary (means over attacks) by Model  Category ===\")\n",
        "display(grouped)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "FsmaOncdsCwc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- FIXED grouped summary (robust to pandas versions) ---\n",
        "# Recompute from `df` you already built\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Ensure numeric dtypes\n",
        "for c in [\"clean_macro_f1\",\"attacked_macro_f1\",\"delta_macro_f1\",\"ASR\"]:\n",
        "    df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
        "\n",
        "grouped = (\n",
        "    df.groupby([\"model\",\"category\"], observed=False)\n",
        "      .agg({\n",
        "          \"clean_macro_f1\": \"mean\",\n",
        "          \"attacked_macro_f1\": \"mean\",\n",
        "          \"delta_macro_f1\": \"mean\",\n",
        "          \"ASR\": \"mean\",\n",
        "          \"attack\": \"nunique\"\n",
        "      })\n",
        "      .rename(columns={\n",
        "          \"clean_macro_f1\": \"mean_clean_macro_f1\",\n",
        "          \"attacked_macro_f1\": \"mean_attacked_macro_f1\",\n",
        "          \"delta_macro_f1\": \"mean_delta_macro_f1\",\n",
        "          \"ASR\": \"mean_ASR\",\n",
        "          \"attack\": \"n_attacks\"\n",
        "      })\n",
        "      .reset_index()\n",
        "      .sort_values([\"model\",\"category\"])\n",
        ")\n",
        "\n",
        "# Round for display\n",
        "for c in [\"mean_clean_macro_f1\",\"mean_attacked_macro_f1\",\"mean_delta_macro_f1\",\"mean_ASR\"]:\n",
        "    grouped[c] = grouped[c].round(4)\n",
        "\n",
        "print(\"\\n=== Grouped summary (means over attacks) by Model  Category ===\")\n",
        "display(grouped)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "8yE8CFsqsvu1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# HateEval labeled evaluation (3 models): Baseline, Gated, Sentinel\n",
        "# Saves predictions + metrics; prints tidy DataFrames\n",
        "# ================================\n",
        "!pip -q install transformers datasets pandas scikit-learn\n",
        "\n",
        "import os, re, math, json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from typing import List, Dict, Any\n",
        "from datasets import Dataset\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_recall_fscore_support,\n",
        "    classification_report, roc_auc_score, confusion_matrix\n",
        ")\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoModel, AutoConfig, AutoModelForSequenceClassification,\n",
        "    PretrainedConfig, PreTrainedModel, TrainingArguments, Trainer\n",
        ")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# -------- Paths (edit if different) --------\n",
        "BASE = \"/content/drive/MyDrive/hate\"\n",
        "TEST_CSV = f\"{BASE}/hateEval_test.csv\"     # must have: id,text,label (label in {0,1})\n",
        "CKPT_BASELINE = f\"{BASE}/models/step1_bert_baseline\"\n",
        "CKPT_GATED    = f\"{BASE}/models/step1_gated_fusion\"\n",
        "CKPT_SENTINEL = f\"{BASE}/models/sentinel_xlmr\"\n",
        "OUT_DIR       = f\"{BASE}/predictions\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "# -------- Load HateEval test ----------\n",
        "df = pd.read_csv(TEST_CSV)\n",
        "assert {\"id\",\"text\",\"label\"}.issubset(df.columns), \"hateEval_test.csv must have columns: id,text,label\"\n",
        "# Coerce labels to {0,1}\n",
        "if df[\"label\"].dtype != int and df[\"label\"].dtype != np.int64:\n",
        "    df[\"label\"] = df[\"label\"].astype(str).str.strip().map({\"0\":0,\"1\":1,\"non-hate\":0,\"hate\":1}).astype(int)\n",
        "\n",
        "texts = df[\"text\"].astype(str).tolist()\n",
        "ids   = df[\"id\"].tolist()\n",
        "y_true = df[\"label\"].astype(int).values\n",
        "\n",
        "# =======================\n",
        "# Gated-fusion definition\n",
        "# =======================\n",
        "class GFConfig(PretrainedConfig):\n",
        "    model_type = \"gated_fusion_wrapper\"\n",
        "    def __init__(self, base_model_name=\"bert-base-multilingual-cased\", num_labels=2, gate_hidden=256, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.base_model_name = base_model_name\n",
        "        self.num_labels = num_labels\n",
        "        self.gate_hidden = gate_hidden\n",
        "\n",
        "class GatedFusionForSequenceClassification(PreTrainedModel):\n",
        "    config_class = GFConfig\n",
        "    def __init__(self, config: GFConfig):\n",
        "        super().__init__(config)\n",
        "        self.base_cfg = AutoConfig.from_pretrained(config.base_model_name)\n",
        "        self.encoder = AutoModel.from_pretrained(config.base_model_name, config=self.base_cfg)\n",
        "        hidden = self.base_cfg.hidden_size\n",
        "        self.gate_mlp = nn.Sequential(\n",
        "            nn.Linear(2 * hidden, config.gate_hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(config.gate_hidden, hidden),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        self.dropout = nn.Dropout(getattr(self.base_cfg, \"hidden_dropout_prob\", 0.1))\n",
        "        self.classifier = nn.Linear(hidden, config.num_labels)\n",
        "        self.post_init()\n",
        "\n",
        "    @staticmethod\n",
        "    def masked_mean(last_hidden_state, attention_mask):\n",
        "        mask = attention_mask.unsqueeze(-1).type_as(last_hidden_state)\n",
        "        summed = (last_hidden_state * mask).sum(dim=1)\n",
        "        denom = mask.sum(dim=1).clamp(min=1e-6)\n",
        "        return summed / denom\n",
        "\n",
        "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, labels=None, **kwargs):\n",
        "        allowed = {\"position_ids\",\"head_mask\",\"inputs_embeds\",\"output_attentions\",\n",
        "                   \"output_hidden_states\",\"return_dict\",\"past_key_values\",\n",
        "                   \"encoder_hidden_states\",\"encoder_attention_mask\"}\n",
        "        safe_kwargs = {k:v for k,v in kwargs.items() if k in allowed}\n",
        "        safe_kwargs.pop(\"num_items_in_batch\", None)\n",
        "\n",
        "        enc = self.encoder(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, **safe_kwargs)\n",
        "        h_cls = enc.last_hidden_state[:, 0, :]\n",
        "        h_mean = self.masked_mean(enc.last_hidden_state, attention_mask)\n",
        "        g = self.gate_mlp(torch.cat([h_cls, h_mean], dim=-1))\n",
        "        fused = g * h_cls + (1.0 - g) * h_mean\n",
        "        logits = self.classifier(self.dropout(fused))\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss = nn.CrossEntropyLoss()(logits.view(-1, self.config.num_labels), labels.view(-1))\n",
        "        return {\"loss\": loss, \"logits\": logits}\n",
        "\n",
        "# ===================\n",
        "# Sentinel definition\n",
        "# ===================\n",
        "SLUR_REGEXES = [\n",
        "    r\"\\bmongol(s)?\\b\", r\"\\bretard(s|ed)?\\b\", r\"\\btolol\\b\", r\"\\bkontol\\b\",\n",
        "    r\"\\bbajingan\\b\", r\"\\bbabi\\b\", r\"\\bbhen ?chod\\b\", r\"\\bmadar ?chod\\b\",\n",
        "    r\"\\brandi\\b\", r\"\\bperra\\b\", r\"\\bzorra\\b\", r\"\\bputa\\b\"\n",
        "]\n",
        "SLUR_PATTERNS = [re.compile(p, re.IGNORECASE) for p in SLUR_REGEXES]\n",
        "\n",
        "def build_heuristic_features(text: str, H=32) -> np.ndarray:\n",
        "    length = len(text)\n",
        "    words  = text.split()\n",
        "    n_words = max(1, len(words))\n",
        "    upper = sum(1 for c in text if c.isalpha() and c.isupper())\n",
        "    digits = sum(1 for c in text if c.isdigit())\n",
        "    punct = sum(1 for c in text if c in \".,;:!?\")\n",
        "    upper_ratio = upper / max(1, sum(c.isalpha() for c in text))\n",
        "    digit_ratio = digits / max(1, len(text))\n",
        "    punct_ratio = punct / max(1, len(text))\n",
        "    slur_hits = sum(len(pat.findall(text)) for pat in SLUR_PATTERNS)\n",
        "    slur_density = slur_hits / n_words\n",
        "    cues = sum(text.lower().count(k) for k in [\"kill\",\"die\",\"trash\",\"dirty\",\"dog\",\"pig\",\"scum\",\"hate\"])\n",
        "    cue_density = cues / n_words\n",
        "    base_feats = np.array([\n",
        "        length, n_words, upper, digits, punct,\n",
        "        upper_ratio, digit_ratio, punct_ratio,\n",
        "        slur_hits, slur_density, cues, cue_density\n",
        "    ], dtype=np.float32)\n",
        "    base_feats[0] = math.log1p(base_feats[0])\n",
        "    base_feats[1] = math.log1p(base_feats[1])\n",
        "    if base_feats.shape[0] < H:\n",
        "        pad = np.zeros(H - base_feats.shape[0], dtype=np.float32)\n",
        "        feats = np.concatenate([base_feats, pad])\n",
        "    else:\n",
        "        feats = base_feats[:H]\n",
        "    return feats\n",
        "\n",
        "class SentinelConfig(PretrainedConfig):\n",
        "    model_type = \"sentinel_fusion\"\n",
        "    def __init__(self,\n",
        "        base_model_name=\"xlm-roberta-base\",\n",
        "        num_labels=2,\n",
        "        heuristic_dim=32,\n",
        "        heuristic_hidden=256,\n",
        "        causal_hidden=256,\n",
        "        attn_heads=8,\n",
        "        aux_causal_loss_weight=0.0,\n",
        "        **kwargs\n",
        "    ):\n",
        "        super().__init__(**kwargs)\n",
        "        self.base_model_name = base_model_name\n",
        "        self.num_labels = num_labels\n",
        "        self.heuristic_dim = heuristic_dim\n",
        "        self.heuristic_hidden = heuristic_hidden\n",
        "        self.causal_hidden = causal_hidden\n",
        "        self.attn_heads = attn_heads\n",
        "        self.aux_causal_loss_weight = aux_causal_loss_weight\n",
        "\n",
        "class SentinelModel(PreTrainedModel):\n",
        "    config_class = SentinelConfig\n",
        "    def __init__(self, config: SentinelConfig):\n",
        "        super().__init__(config)\n",
        "        self.base_cfg = AutoConfig.from_pretrained(config.base_model_name)\n",
        "        self.encoder = AutoModel.from_pretrained(config.base_model_name, config=self.base_cfg)\n",
        "        hidden = self.base_cfg.hidden_size\n",
        "\n",
        "        self.heuristic_proj = nn.Sequential(\n",
        "            nn.Linear(config.heuristic_dim, config.heuristic_hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(config.heuristic_hidden, hidden),\n",
        "            nn.LayerNorm(hidden)\n",
        "        )\n",
        "        self.causal_mlp = nn.Sequential(\n",
        "            nn.Linear(hidden, config.causal_hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(config.causal_hidden, hidden),\n",
        "            nn.LayerNorm(hidden)\n",
        "        )\n",
        "        self.xattn = nn.MultiheadAttention(embed_dim=hidden, num_heads=config.attn_heads, batch_first=True)\n",
        "        self.dropout = nn.Dropout(getattr(self.base_cfg, \"hidden_dropout_prob\", 0.1))\n",
        "        self.classifier = nn.Linear(hidden, config.num_labels)\n",
        "        self.post_init()\n",
        "\n",
        "    def forward(self, input_ids=None, attention_mask=None, heuristic_feats=None, labels=None, **kwargs):\n",
        "        enc = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        last_hidden = enc.last_hidden_state\n",
        "        h_cls = last_hidden[:, 0, :]\n",
        "        h_heu = self.heuristic_proj(heuristic_feats)\n",
        "        h_cau = self.causal_mlp(h_cls)\n",
        "        Q = h_cls.unsqueeze(1)\n",
        "        KV = torch.stack([h_heu, h_cau], dim=1)\n",
        "        fused, _ = self.xattn(Q, KV, KV)\n",
        "        fused = fused.squeeze(1)\n",
        "        logits = self.classifier(self.dropout(fused))\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss = nn.CrossEntropyLoss()(logits, labels)\n",
        "        return {\"loss\": loss, \"logits\": logits}\n",
        "\n",
        "# -------------------------\n",
        "# Helper: softmax to probs\n",
        "# -------------------------\n",
        "def to_probs(logits: np.ndarray) -> np.ndarray:\n",
        "    e = np.exp(logits - logits.max(axis=1, keepdims=True))\n",
        "    return e / e.sum(axis=1, keepdims=True)\n",
        "\n",
        "# ----------------------------------\n",
        "# Predictors for each model family\n",
        "# ----------------------------------\n",
        "def predict_probs_baseline(checkpoint_dir: str, texts: List[str], max_length=128) -> np.ndarray:\n",
        "    tok = AutoTokenizer.from_pretrained(checkpoint_dir if os.path.isdir(checkpoint_dir) else \"bert-base-multilingual-cased\")\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(checkpoint_dir).to(device).eval()\n",
        "    enc = tok(texts, padding=\"max_length\", truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
        "    ds = Dataset.from_dict({k: enc[k].tolist() for k in enc})\n",
        "    ds.set_format(type=\"torch\", columns=[\"input_ids\",\"attention_mask\"])\n",
        "    args = TrainingArguments(output_dir=\"./tmp_eval\", per_device_eval_batch_size=64, report_to=\"none\", logging_strategy=\"no\", disable_tqdm=False)\n",
        "    trainer = Trainer(model=model, args=args, tokenizer=tok)\n",
        "    with torch.no_grad():\n",
        "        out = trainer.predict(ds)\n",
        "    return to_probs(out.predictions)\n",
        "\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer, AutoConfig, AutoModel\n",
        "from transformers import PretrainedConfig, PreTrainedModel\n",
        "\n",
        "# --- keep your GFConfig and GatedFusionForSequenceClassification definitions as-is ---\n",
        "\n",
        "def _softmax_np(logits: np.ndarray) -> np.ndarray:\n",
        "    e = np.exp(logits - logits.max(axis=1, keepdims=True))\n",
        "    return e / e.sum(axis=1, keepdims=True)\n",
        "\n",
        "@torch.no_grad()\n",
        "def predict_probs_gated(checkpoint_dir: str, texts, max_length: int = 128, batch_size: int = 64) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Manual batched inference for the gated model to avoid Trainer/accelerate\n",
        "    touching a dict with loss=None.\n",
        "    \"\"\"\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    base_name = \"bert-base-multilingual-cased\"\n",
        "\n",
        "    # tokenizer\n",
        "    tok = AutoTokenizer.from_pretrained(checkpoint_dir if os.path.isdir(checkpoint_dir) else base_name)\n",
        "\n",
        "    # config + model\n",
        "    try:\n",
        "        cfg = GFConfig.from_pretrained(checkpoint_dir)\n",
        "    except Exception:\n",
        "        cfg = GFConfig(base_model_name=base_name, num_labels=2, gate_hidden=256)\n",
        "    model = GatedFusionForSequenceClassification(cfg)\n",
        "    state_path = os.path.join(checkpoint_dir, \"pytorch_model.bin\")\n",
        "    if os.path.isfile(state_path):\n",
        "        state = torch.load(state_path, map_location=device)\n",
        "        model.load_state_dict(state, strict=False)\n",
        "    model.to(device).eval()\n",
        "\n",
        "    # batched forward\n",
        "    probs_list = []\n",
        "    N = len(texts)\n",
        "    for i in range(0, N, batch_size):\n",
        "        batch_texts = texts[i:i+batch_size]\n",
        "        enc = tok(batch_texts, padding=\"max_length\", truncation=True,\n",
        "                  max_length=max_length, return_tensors=\"pt\")\n",
        "        enc = {k: v.to(device) for k, v in enc.items()}\n",
        "        out = model(**enc)\n",
        "        # out is a dict {\"loss\": None/..., \"logits\": tensor}\n",
        "        logits = out[\"logits\"].detach().cpu().numpy()\n",
        "        probs_list.append(_softmax_np(logits))\n",
        "    return np.vstack(probs_list)\n",
        "\n",
        "# ===== Optional: make Sentinel manual too (more robust and symmetric) =====\n",
        "\n",
        "# keep your SentinelConfig and SentinelModel definitions as-is\n",
        "def _build_heuristic_batch(texts, H=32):\n",
        "    arr = np.stack([build_heuristic_features(t, H=H) for t in texts]).astype(np.float32)\n",
        "    return torch.tensor(arr)\n",
        "\n",
        "@torch.no_grad()\n",
        "def predict_probs_sentinel(checkpoint_dir: str, texts, max_length: int = 128, batch_size: int = 64) -> np.ndarray:\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    base_name = \"xlm-roberta-base\"\n",
        "\n",
        "    tok = AutoTokenizer.from_pretrained(checkpoint_dir if os.path.isdir(checkpoint_dir) else base_name)\n",
        "    try:\n",
        "        scfg = SentinelConfig.from_pretrained(checkpoint_dir)\n",
        "    except Exception:\n",
        "        scfg = SentinelConfig(base_model_name=base_name, num_labels=2, heuristic_dim=32)\n",
        "\n",
        "    model = SentinelModel(scfg)\n",
        "    state_path = os.path.join(checkpoint_dir, \"pytorch_model.bin\")\n",
        "    if os.path.isfile(state_path):\n",
        "        state = torch.load(state_path, map_location=device)\n",
        "        model.load_state_dict(state, strict=False)\n",
        "    model.to(device).eval()\n",
        "\n",
        "    probs_list = []\n",
        "    N = len(texts)\n",
        "    for i in range(0, N, batch_size):\n",
        "        batch_texts = texts[i:i+batch_size]\n",
        "        enc = tok(batch_texts, padding=\"max_length\", truncation=True,\n",
        "                  max_length=max_length, return_tensors=\"pt\")\n",
        "        heur = _build_heuristic_batch(batch_texts, H=scfg.heuristic_dim)\n",
        "\n",
        "        enc = {k: v.to(device) for k, v in enc.items()}\n",
        "        enc[\"heuristic_feats\"] = heur.to(device)\n",
        "\n",
        "        out = model(**enc)\n",
        "        logits = out[\"logits\"].detach().cpu().numpy()\n",
        "        probs_list.append(_softmax_np(logits))\n",
        "    return np.vstack(probs_list)\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Run predictions\n",
        "# -------------------------\n",
        "print(\"Scoring HateEval with Baseline...\")\n",
        "probs_base = predict_probs_baseline(CKPT_BASELINE, texts)\n",
        "print(\"Scoring HateEval with Gated Fusion...\")\n",
        "probs_gated = predict_probs_gated(CKPT_GATED, texts)\n",
        "print(\"Scoring HateEval with Sentinel...\")\n",
        "probs_sent = predict_probs_sentinel(CKPT_SENTINEL, texts)\n",
        "\n",
        "def save_preds(name, probs, y_true):\n",
        "    preds = probs.argmax(axis=1)\n",
        "    out = pd.DataFrame({\n",
        "        \"id\": ids,\n",
        "        \"text\": texts,\n",
        "        \"label\": y_true,\n",
        "        \"prob_nonhate\": probs[:,0],\n",
        "        \"prob_hate\": probs[:,1],\n",
        "        \"pred_label\": preds\n",
        "    })\n",
        "    path = os.path.join(OUT_DIR, f\"hateEval_{name}.csv\")\n",
        "    out.to_csv(path, index=False)\n",
        "    print(f\"Saved predictions: {path}\")\n",
        "    return preds, out\n",
        "\n",
        "pred_base , df_base  = save_preds(\"baseline\", probs_base, y_true)\n",
        "pred_gated, df_gated = save_preds(\"gated\",    probs_gated, y_true)\n",
        "pred_sent , df_sent  = save_preds(\"sentinel\", probs_sent, y_true)\n",
        "\n",
        "# -------------------------\n",
        "# Metrics + Reports\n",
        "# -------------------------\n",
        "def compute_all_metrics(name: str, y_true, probs):\n",
        "    y_pred = probs.argmax(axis=1)\n",
        "\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    p_macro, r_macro, f1_macro, _ = precision_recall_fscore_support(\n",
        "        y_true, y_pred, average=\"macro\", zero_division=0\n",
        "    )\n",
        "\n",
        "    # per-class (labels assumed 0=non-hate, 1=hate)\n",
        "    p_c, r_c, f1_c, sup_c = precision_recall_fscore_support(\n",
        "        y_true, y_pred, labels=[0, 1], zero_division=0\n",
        "    )\n",
        "\n",
        "    # ROC-AUC for positive class (handle degenerate cases)\n",
        "    try:\n",
        "        auc = roc_auc_score(y_true, probs[:, 1])  # hate=1\n",
        "    except ValueError:\n",
        "        auc = float(\"nan\")\n",
        "\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1]).tolist()\n",
        "\n",
        "    rep = {\n",
        "        \"model\": name,\n",
        "        \"accuracy\": float(acc),\n",
        "        \"precision_macro\": float(p_macro),\n",
        "        \"recall_macro\": float(r_macro),\n",
        "        \"f1_macro\": float(f1_macro),\n",
        "        \"auc_roc_hate1\": float(auc) if auc == auc else None,  # NaN -> None for JSON\n",
        "        \"per_class\": [\n",
        "            {\n",
        "                \"label\": 0,\n",
        "                \"precision\": float(p_c[0]),\n",
        "                \"recall\": float(r_c[0]),\n",
        "                \"f1\": float(f1_c[0]),\n",
        "                \"support\": int(sup_c[0]),\n",
        "            },\n",
        "            {\n",
        "                \"label\": 1,\n",
        "                \"precision\": float(p_c[1]),\n",
        "                \"recall\": float(r_c[1]),\n",
        "                \"f1\": float(f1_c[1]),\n",
        "                \"support\": int(sup_c[1]),\n",
        "            },\n",
        "        ],\n",
        "        \"confusion_matrix\": {\"labels\": [0, 1], \"matrix\": cm},\n",
        "    }\n",
        "    return rep\n",
        "\n",
        "rep_base  = compute_all_metrics(\"Baseline\",     y_true, probs_base)\n",
        "rep_gated = compute_all_metrics(\"Gated Fusion\", y_true, probs_gated)\n",
        "rep_sent  = compute_all_metrics(\"Sentinel\",     y_true, probs_sent)\n",
        "\n",
        "# Save JSON reports\n",
        "for rep, nm in [(rep_base, \"baseline\"), (rep_gated, \"gated\"), (rep_sent, \"sentinel\")]:\n",
        "    with open(os.path.join(OUT_DIR, f\"hateEval_metrics_{nm}.json\"), \"w\") as f:\n",
        "        json.dump(rep, f, indent=2)\n",
        "\n",
        "# Tidy summary table (fix: build rows in a simple loop)\n",
        "rows = []\n",
        "for r in (rep_base, rep_gated, rep_sent):\n",
        "    rows.append({k: v for k, v in r.items() if k not in (\"per_class\", \"confusion_matrix\")})\n",
        "summary = pd.DataFrame(rows)\n",
        "\n",
        "# Select and order common columns if present\n",
        "ordered_cols = [\"model\", \"accuracy\", \"precision_macro\", \"recall_macro\", \"f1_macro\", \"auc_roc_hate1\"]\n",
        "summary = summary[ordered_cols]\n",
        "\n",
        "# Round numeric columns\n",
        "summary_rounded = summary.copy()\n",
        "for col in ordered_cols:\n",
        "    if col != \"model\":\n",
        "        summary_rounded[col] = summary_rounded[col].apply(lambda x: None if x is None else round(float(x), 4))\n",
        "\n",
        "# Per-class tidy\n",
        "def per_class_rows(rep):\n",
        "    return [\n",
        "        {\n",
        "            \"model\": rep[\"model\"],\n",
        "            \"label\": d[\"label\"],\n",
        "            \"precision\": float(d[\"precision\"]),\n",
        "            \"recall\": float(d[\"recall\"]),\n",
        "            \"f1\": float(d[\"f1\"]),\n",
        "            \"support\": int(d[\"support\"]),\n",
        "        }\n",
        "        for d in rep[\"per_class\"]\n",
        "    ]\n",
        "\n",
        "per_class_df = pd.DataFrame(per_class_rows(rep_base) + per_class_rows(rep_gated) + per_class_rows(rep_sent))\n",
        "per_class_df_rounded = per_class_df.copy()\n",
        "for c in [\"precision\", \"recall\", \"f1\"]:\n",
        "    per_class_df_rounded[c] = per_class_df_rounded[c].map(lambda x: round(float(x), 4))\n",
        "\n",
        "# Save CSVs\n",
        "summary_rounded.to_csv(os.path.join(OUT_DIR, \"hateEval_summary_metrics.csv\"), index=False)\n",
        "per_class_df_rounded.to_csv(os.path.join(OUT_DIR, \"hateEval_per_class_metrics.csv\"), index=False)\n",
        "\n",
        "# -------- Pretty print to notebook --------\n",
        "print(\"\\n=== HateEval: Summary Metrics ===\")\n",
        "print(summary_rounded.to_string(index=False))\n",
        "\n",
        "print(\"\\n=== HateEval: Per-Class Metrics ===\")\n",
        "print(per_class_df_rounded.sort_values([\"label\", \"model\"]).to_string(index=False))\n",
        "\n",
        "print(f\"\\nArtifacts saved under: {OUT_DIR}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "vmoPGePM1O7A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %% Dataset composition summary for HateXplain (train/val/test) and HateEval\n",
        "import os\n",
        "import pandas as pd\n",
        "from collections import OrderedDict\n",
        "\n",
        "# ----- paths (edit if yours differ) -----\n",
        "DRIVE = \"/content/drive/MyDrive/hate\"\n",
        "PATHS = OrderedDict({\n",
        "    \"HateXplain\": {\n",
        "        \"train\": os.path.join(DRIVE, \"train.csv\"),\n",
        "        \"validation\": os.path.join(DRIVE, \"val.csv\"),\n",
        "        \"test\": os.path.join(DRIVE, \"test.csv\"),\n",
        "    },\n",
        "    # Well try hateEval_test.csv first; fallback to hateEval.csv if thats what you saved.\n",
        "    \"HateEval\": {\n",
        "        \"test\": os.path.join(DRIVE, \"hateEval_test.csv\") if os.path.exists(os.path.join(DRIVE, \"hateEval_test.csv\"))\n",
        "                else os.path.join(DRIVE, \"hateEval.csv\")\n",
        "    },\n",
        "})\n",
        "\n",
        "# ----- helpers -----\n",
        "POSSIBLE_LABEL_COLS = [\"label\", \"labels\", \"target\", \"class\", \"y\"]\n",
        "POSSIBLE_TEXT_COLS  = [\"text\", \"tweet\", \"content\", \"document\", \"sentence\"]\n",
        "\n",
        "def load_with_autocols(path):\n",
        "    \"\"\"Load CSV and auto-detect label/text columns; return (df, label_col, text_col).\"\"\"\n",
        "    df = pd.read_csv(path)\n",
        "    label_col = next((c for c in POSSIBLE_LABEL_COLS if c in df.columns), None)\n",
        "    text_col  = next((c for c in POSSIBLE_TEXT_COLS  if c in df.columns), None)\n",
        "    if label_col is None:\n",
        "        raise ValueError(f\"No label column found in {path}. \"\n",
        "                         f\"Expected one of {POSSIBLE_LABEL_COLS}, got {list(df.columns)}\")\n",
        "    if text_col is None:\n",
        "        raise ValueError(f\"No text column found in {path}. \"\n",
        "                         f\"Expected one of {POSSIBLE_TEXT_COLS}, got {list(df.columns)}\")\n",
        "    return df, label_col, text_col\n",
        "\n",
        "def summarize_split(df, label_col, dataset_name, split_name):\n",
        "    \"\"\"Return long-form and wide-form summaries for one split.\"\"\"\n",
        "    # Ensure labels are simple (0/1 or strings); dont coerce to int to avoid crashing on strings.\n",
        "    counts = (\n",
        "        df.groupby(label_col, dropna=False)\n",
        "          .size()\n",
        "          .reset_index(name=\"count\")\n",
        "          .rename(columns={label_col: \"label\"})\n",
        "    )\n",
        "    counts[\"dataset\"] = dataset_name\n",
        "    counts[\"split\"]   = split_name\n",
        "    # Percent within split\n",
        "    total = counts[\"count\"].sum()\n",
        "    counts[\"percent\"] = counts[\"count\"] / max(total, 1) * 100.0\n",
        "\n",
        "    # Wide view: one row per split with columns for each labels count\n",
        "    wide = counts.pivot_table(index=[\"dataset\", \"split\"], columns=\"label\", values=\"count\", fill_value=0)\n",
        "    wide = wide.reset_index()\n",
        "    wide.columns.name = None\n",
        "    wide[\"total\"] = wide.drop(columns=[\"dataset\", \"split\"]).sum(axis=1)\n",
        "\n",
        "    # If binary {0,1}, add class balance stats\n",
        "    if 0 in counts[\"label\"].unique().tolist() and 1 in counts[\"label\"].unique().tolist():\n",
        "        # Retrieve counts safely (may be missing in some splits)\n",
        "        def _get(w, col):\n",
        "            return w[col] if col in w else 0\n",
        "        wide[\"pos_frac_%\"] = wide.apply(lambda r: ( _get(r, 1) / r[\"total\"] * 100.0 ) if r[\"total\"] > 0 else 0.0, axis=1)\n",
        "        wide[\"neg_frac_%\"] = wide.apply(lambda r: ( _get(r, 0) / r[\"total\"] * 100.0 ) if r[\"total\"] > 0 else 0.0, axis=1)\n",
        "\n",
        "    # Sort label columns (nice ordering)\n",
        "    non_label_cols = [\"dataset\", \"split\", \"total\", \"pos_frac_%\", \"neg_frac_%\"]\n",
        "    non_label_cols = [c for c in non_label_cols if c in wide.columns]\n",
        "    label_cols = [c for c in wide.columns if c not in non_label_cols]\n",
        "    # Keep dataset/split first\n",
        "    wide = wide[[\"dataset\", \"split\"] + label_cols + [c for c in non_label_cols if c not in [\"dataset\",\"split\"]]]\n",
        "\n",
        "    return counts, wide\n",
        "\n",
        "# ----- build summaries -----\n",
        "all_long = []\n",
        "all_wide = []\n",
        "\n",
        "for ds_name, splits in PATHS.items():\n",
        "    for split_name, p in splits.items():\n",
        "        if not os.path.exists(p):\n",
        "            print(f\"[WARN] Missing file for {ds_name}/{split_name}: {p}\")\n",
        "            continue\n",
        "        df, label_col, text_col = load_with_autocols(p)\n",
        "        long_df, wide_df = summarize_split(df, label_col, ds_name, split_name)\n",
        "        all_long.append(long_df)\n",
        "        all_wide.append(wide_df)\n",
        "\n",
        "if not all_long:\n",
        "    raise RuntimeError(\"No datasets were found. Check PATHS.\")\n",
        "\n",
        "long_summary = pd.concat(all_long, ignore_index=True)\n",
        "wide_summary = pd.concat(all_wide, ignore_index=True)\n",
        "\n",
        "# Pretty sort\n",
        "long_summary = long_summary.sort_values([\"dataset\", \"split\", \"label\"]).reset_index(drop=True)\n",
        "wide_summary = wide_summary.sort_values([\"dataset\", \"split\"]).reset_index(drop=True)\n",
        "\n",
        "# Round percents\n",
        "if \"percent\" in long_summary.columns:\n",
        "    long_summary[\"percent\"] = long_summary[\"percent\"].map(lambda x: round(float(x), 2))\n",
        "for col in [\"pos_frac_%\", \"neg_frac_%\"]:\n",
        "    if col in wide_summary.columns:\n",
        "        wide_summary[col] = wide_summary[col].map(lambda x: round(float(x), 2))\n",
        "\n",
        "# ----- display -----\n",
        "print(\"\\n=== CLASS COMPOSITION (long-form: one row per label) ===\")\n",
        "display(long_summary)\n",
        "\n",
        "print(\"\\n=== SPLIT SUMMARY (wide-form: counts by label + totals) ===\")\n",
        "display(wide_summary)\n",
        "\n",
        "# ----- save to Drive for record -----\n",
        "out_dir = os.path.join(DRIVE, \"analysis\")\n",
        "os.makedirs(out_dir, exist_ok=True)\n",
        "long_summary.to_csv(os.path.join(out_dir, \"dataset_composition_long.csv\"), index=False)\n",
        "wide_summary.to_csv(os.path.join(out_dir, \"dataset_composition_wide.csv\"), index=False)\n",
        "print(f\"\\nSaved CSVs to: {out_dir}\")\n"
      ],
      "metadata": {
        "id": "sgK7oHEifewj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}